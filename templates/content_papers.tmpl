<div class="content-container">
<div class="jumbotron">
   <h1>
      Accepted Talks<br />
      <small>North East Database Day 2025 </small><br />
      <small>Thursday January 9th, 2025</small><br/>
      <small><a href="https://www.bu.edu/cds-faculty/explore/bu-center-for-computing-data-sciences/" target="_blank">Center for Computing & Data Sciences (CCDS)</a> 1750 @ Boston University</small>
   </h1>
</div>
   <p>


   <h3>List of accepted talks:</h3>


	<p style="text-align: center;">
	<a href="javascript: toggleVisibility ('showall')">Show</a>/<a href="javascript: toggleVisibility ('hideall')">Hide all abstracts</a>
	</p>

   <table class="table table-bordered posterTable">
      <tbody>

         <tr>
          <td>
              Jianguo Wang (Purdue University)*<br/>
              <strong>Specialized vs. Generalized Vector Databases: How Different Are They Really?</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po0')">Toggle Abstract</a>
              <div id="po0" class="abstract" style="display: none;">
                <p>
                Recently, there has been a tremendous surge of interest in building vector databases. Existing vector databases can be broadly classified into two categories: specialized and generalized vector databases. Our experiments demonstrate a significant performance slowdown in generalized vector databases compared to specialized ones. This outcome is expected, as specialized vector databases can leverage optimizations not available to generalized vector databases. But what exactly are the root causes? Is it possible to bridge the performance gap? If so, how?

                In this talk, I will present our research answering these questions. This talk is based on a collection of papers published or submitted: ICDE'24, VLDB'24 (submitted), TODS'24 (submitted), and SIGMOD'21.
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Geoffrey X. Yu (Massachusetts Institute of Technology)*; Ziniu Wu (Massachusetts Institute of Technology); Ferdinand Kossmann (MIT); Tianyu Li (MIT CSAIL); Markos Markakis (Massachusetts Institute of Technology); Amadou L Ngom (MIT); Tim Kraska (MIT); Samuel Madden (MIT)<br/>
              <strong>Virtualizing Cloud Data Infrastructures with BRAD</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po1')">Toggle Abstract</a>
              <div id="po1" class="abstract" style="display: none;">
                <p>
                In the cloud, organizations typically manage their data using a mishmash of specialized database engines (e.g., Aurora, BigQuery, etc.), each chosen for the functionality and performance/cost benefits they bring to different parts of their workload. Realizing and managing these multi-engine infrastructures is hard. There can be multiple possible designs, each with subtly different performance and costs. Moreover, changing the design later on (e.g., due to business growth) is challenging as application code usually ends up tightly coupled to the chosen physical infrastructure. We solve these problems using virtual database engines (VDBEs), a novel abstraction that we implement in BRAD: a new system that automates data infrastructure design and management. VDBEs virtualize a data infrastructure to separate the concerns of logically specifying the performance/functionality that applications need from physically realizing them onto concrete engines. Users declaratively define a set of VDBEs, which each describe the “properties” that an application expects from an engine (e.g., query interface, performance, data freshness). BRAD then automatically realizes these VDBEs onto physical cloud engines by solving a cost-based optimization problem, leveraging learned performance models. Applications integrate with VDBEs and have no knowledge of the underlying physical engines, enabling BRAD to later modify the physical infrastructure without needing any application code changes. In our talk, we will present the VDBE abstraction and highlight its flexibility by giving examples of how common data infrastructure needs are expressed using VDBEs. We will also discuss how BRAD automatically realizes VDBEs onto a physical infrastructure.

                This talk is based on ongoing work and two papers (one published, one under submission).
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Andrew Pavlo (Carnegie Mellon University)*<br/>
              <strong>Why Machine Learning for Automatically Optimizing Databases Doesn't Work</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po2')">Toggle Abstract</a>
              <div id="po2" class="abstract" style="display: none;">
                <p>
                Database management systems (DBMSs) are complex software that requires sophisticated tuning to work efficiently for a given workload and operating environment. Such tuning requires considerable effort from experienced administrators, which is not scalable for large DBMS fleets. This problem has led to research on using machine learning (ML) to devise strategies to optimize DBMS configurations for any application, including automatic physical database design, knob configuration, and query tuning. Despite the many academic papers that tout the benefits of using ML to optimize databases, there have been only a few major success stories in industry in the last decade.

                In this talk, I discuss the challenges of using ML-enhanced tuning methods to optimize databases. I will address specific assumptions that researchers make about production database environments that are incorrect and identify why ML is not always the best solution to solving real-world database problems.
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Roee Shraga (WPI)*<br/>
              <strong>Semantic Version Management in Data Lakes</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po3')">Toggle Abstract</a>
              <div id="po3" class="abstract" style="display: none;">
                <p>
				*The talk is based on a published paper and open problems that follow*
				In this talk, I will present our new project on Semantic Version Management in which we seek to lay the foundations for semantic understanding of data changes that result in multiple versions and introduce scalable tools to uncover and explain data changes. This project is supported by NSF award number IIS-2325632. Specifically, I will introduce Explain-Da-V (a VLDB23 Paper), a framework that explains changes between two given dataset versions using data transformations. I will present open problems and challenges that include efficiently finding different versions of a dataset from within a massive table repository and understanding the version history among a collection of versions.
				This is a joint work with Renee Miller from Northeastern University. 
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Saurabh Bajaj (University of Massachusetts, Amherst)*; Hui Guan (University of Massachusetts, Amherst); Marco Serafini (University of Massachusetts Amherst)<br/>
              <strong>GNN Training Systems: Comparison of Full-Graph and Mini-Batch</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po4')">Toggle Abstract</a>
              <div id="po4" class="abstract" style="display: none;">
                <p>
				Graph Neural Networks (GNNs) have gained significant
              attention in recent years due to their ability to learn representations of graph-structured data. Two common methods for training GNNs are mini-batch training and full-graph training. 
              Since these two methods require different training pipelines and systems optimizations, two separate categories of GNN training systems emerged, each tailored for one method only. 
              Previous research introducing systems belonging to a particular category predominantly compares them with other systems within the same category, offering limited or no comparison with systems from the other category.
              Some prior work also argues that one training method is superior to the other to justify their focus on that method, but the existing evidence is incomplete and contradictory. In this paper, we empirically compare the two training methods and GNN training systems that support them in terms of accuracy and performance.
              Our experiments demonstrate that both training techniques converge to similar accuracy values across multiple datasets and models, with a slight advantage for full-graph training.
              Mini-batch training is less sensitive to model architecture changes, making hyperparameter search easier.
              The mini-batch training systems we consider consistently converge faster than the full-graph training ones across multiple datasets, GNN models, and system configurations, between 2.4x - 15.2x faster.

              The talk is about my current work.
                </p>
              </div>
          </td>
        </tr>
		
		<tr>
          <td>
              Markos Markakis (Massachusetts Institute of Technology)*; An Bo Chen (MIT CSAIL); Brit Youngmann (Technion - Israel institute of technology); Trinity Gao (MIT); Ziyu Zhang (MIT); Rana Shahout (Harvard); Peter Chen (Massachusetts Institute of Technology); Chunwei Liu (MIT); Ibrahim Sabek (University of Southern California); Michael Cafarella (MIT CSAIL)<br/>
              <strong>Sawmill: From Logs to Causal Diagnosis of Large Systems</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po5')">Toggle Abstract</a>
              <div id="po5" class="abstract" style="display: none;">
                <p>
				Causal analysis is an essential lens for understanding complex system dynamics in domains as varied as medicine, economics and law. Computer systems can exhibit similar complexity, but much of the information that could help subject them to causal analysis is only available in long, messy, semi-structured log files.

				In this work, we want to apply causal reasoning to facilitate large systems debugging. We transform logs into a representation amenable to causal analysis, using methods from data transformation, cleaning, and extraction. We also present algorithms for efficiently using this representation for causal discovery - the task of constructing a causal model of the system from available data. Our framework gives log-derived variables human-understandable names and distills the information in a log file around a user’s chosen causal units (e.g. users or machines), generating appropriate aggregated variables per causal unit. It then exposes these variables through an interactive interface that users can leverage to recover the portion of the causal model relevant to their question. This makes causal inference possible using off-the-shelf tools.

				We evaluate our framework using Sawmill, an open-source prototype implementation, on both real-world and synthetic log datasets and find it to be: 1. Accurate, achieving on average across datasets a mean reciprocal rank of 0.8018 for relevant causes (41.89% better than the next best baseline) and a mean ATE error of 20.27% (compared to 31.26% for the next best baseline); 2. Computationally efficient, requiring a mean of 346.92s (4.30s per MiB of log data) to go from a log file to a quantitative effect, while scaling linearly with log complexity; 3. Humanly efficient, requiring between 6-10 user interactions end-to-end; and 4. Complete, with each component of our framework shown to both add value and to be efficient. A demonstration of Sawmill has been accepted to SIGMOD ’24. A companion video is available online.
                </p>
              </div>
          </td>
        </tr>
		
		<tr>
          <td>
              Mohammad Javad Amiri (Stony Brook University)*<br/>
              <strong>Towards Adaptive Transaction Processing in Untrusted Environments</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po6')">Toggle Abstract</a>
              <div id="po6" class="abstract" style="display: none;">
                <p>
				This talk articulates our vision for a learning-based distributed transaction processing framework that can be deployed in untrustworthy environments. We argue that as novel smart contracts, modern hardware, and new cloud platforms arise, future-proof distributed transaction processing systems need to be designed with full-stack adaptivity in mind. At the application level, a future-proof system must adaptively learn the best-performing transaction processing paradigm and quickly adapt to new hardware and unanticipated workload changes on the fly. Likewise, the consensus layer must dynamically adjust itself to the workloads, faulty conditions, and network configuration while maintaining compatibility with the transaction processing paradigm. At the infrastructure level, cloud providers must enable cross-layer adaptation, which identifies performance bottlenecks and possible attacks, and determines at runtime the degree of resource disaggregation that best meets application requirements. Within this vision of the future, this talk outlines several research challenges together with some preliminary approaches.

                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Tarikul Islam Papon (Boston University)*; Manos Athanassoulis (Boston University)<br/>
              <strong>CAVE: Concurrency-Aware Graph Processing System for SSD</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po7')">Toggle Abstract</a>
              <div id="po7" class="abstract" style="display: none;">
                <p>
				Large-scale graph analytics has become increasingly common in areas like social networks, physical sciences, transportation networks, and recommendation systems. Since many such practical graphs do not fit in main memory, graph analytics performance depends on efficiently utilizing underlying storage devices. These out-of-core graph processing systems employ sharding and sub-graph partitioning to optimize for storage while relying on efficient sequential access of traditional hard disks. However, today’s storage is increasingly based on solid-state drives (SSDs) that exhibit high internal parallelism and efficient random accesses. Yet, state-of-the-art graph processing systems do not explicitly exploit those properties, resulting in subpar performance.
				We develop CAVE, a graph processing engine that exploits the underlying SSD-based storage by harnessing the available device parallelism via carefully selecting which I/Os to graph data can be issued concurrently. Thus, CAVE traverses multiple paths and processes multiple nodes and edges concurrently. We identify two key ways to parallelize graph traversal algorithms based on the graph structure and algorithm: intra-subgraph and inter-subgraph parallelization. To showcase the benefit of our approach, we build within CAVE parallelized versions of five popular graph algorithms (Breadth-First Search, Depth-First Search, Weakly Connected Components, PageRank, Random Walk) that exploit the full bandwidth of the underlying device. CAVE uses a blocked file format based on adjacency lists and employs a concurrent cache pool. By experimenting with different types of graphs on three SSD devices, we demonstrate that CAVE utilizes the available parallelism, and scales to diverse real-world graph datasets. CAVE achieves up to one order of magnitude speedup compared to the popular out-of-core systems Mosaic and GridGraph, and up to three orders of magnitude speedup in runtime compared to GraphChi.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Douglas B Rumbaugh (Penn State University)*; Dong Xie (Penn State University)<br/>
              <strong>Why aren’t you dynamizing your data structures?</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po8')">Toggle Abstract</a>
              <div id="po8" class="abstract" style="display: none;">
                <p>
				It is not uncommon for the most effective data structure for answering
a particular type of query to be static, but unfortunately update
support is often desirable. This talk will discuss classical theoretical
techniques for automatically adding update support to static structures
(dynamization), the problems that make these techniques unpopular in
practice, and approaches to fixing those problems. Dynamization has
the potential to greatly improve the utility of existing static data
structures, and we'll show a few specific examples of just how powerful
it can be when properly applied, as well as discuss approaches to improve
the performance and generality of the techniques.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Abdul Wasay (Intel)*; Sounak Gupta (Intel Corp); Nesime Tatbul (MIT); David E Cohen (Intel); Orri Erling (Meta Platforms)<br/>
              <strong>Towards an Industry-Scale Feature Engineering Benchmark</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po9')">Toggle Abstract</a>
              <div id="po9" class="abstract" style="display: none;">
                <p>
				Feature engineering is integral to all machine learning pipelines. It is the process through which these pipelines transform raw data into features that are then stored and used for several state-of-the-art machine learning tasks, for instance, training and inferring from deep learning recommendation models (DLRMs) and fine-tuning NLP models through retrieval-augmented generation (RAG). Feature engineering consumes a significant portion of resources at large-scale companies; for instance, it accounts for over 30 percent of the power consumption of production DLRM pipelines at Meta. Feature engineering pipelines at large-scale companies such as Meta, Pinterest, and Amazon exhibit unique properties including a highly disaggregated hardware/software stack, specialized nested data types, massive data about user browsing history, and high refresh rate. Existing benchmarks such as TPC-DS or FEBench do not capture all of these characteristics within the same benchmark. In this talk, we describe a joint effort between Intel, Meta, and MIT to build a feature engineering benchmark that closely captures the scale, data types, and query workloads of data pipelines at various large-scale companies.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Niccolo Meneghetti (University of Michigan - Dearborn)*<br/>
              <strong>StarfishDB: Probabilistic Programming Datalog in Action</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po10')">Toggle Abstract</a>
              <div id="po10" class="abstract" style="display: none;">
                <p>
				StarfishDB is a database-centric probabilistic programming framework, where probabilistic programs are expressed as sets of relational constraints, imposed against a probabilistic database. In this talk we will discuss the design and implementation of StarfishDB, and some of its practical applications. The talk is based on a current research paper, accepted at SIGMOD 2024.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Sivaprasad Sudhir (MIT)*; Jialin Ding (Amazon Web Services); Tim Kraska (MIT); Samuel Madden (MIT)<br/>
              <strong>Self-Organizing Data Containers</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po11')">Toggle Abstract</a>
              <div id="po11" class="abstract" style="display: none;">
                <p>
				We present a new self-organizing, self-optimizing, meta-data-rich storage layer for the cloud called self-organizing data containers (SDCs). SDCs enable order-of-magnitude performance improvements in data-intensive applications through instance optimization, i.e., the adaptation of data representation to exploit both the distribution of the data and the workload running on it. SDCs support modern data storage optimizations at the heart of high-performance data analytics systems, such as indexing, flexible multi-dimensional partitioning and compression, and an ability to adapt to the workloads running on them. Once we have SDCs, many existing
				systems, including conventional relational databases, parallel data processing frameworks, and ML systems, can
				easily adapt SDCs as their storage layer and attain huge performance gains.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Neha Makhija (Northeastern University)*; Wolfgang Gatterbauer (Northeastern University)<br/>
              <strong>Solving Reverse Data Management Problems as Fast as Theoretically Possible: A Unified Framework</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po12')">Toggle Abstract</a>
              <div id="po12" class="abstract" style="display: none;">
                <p>
				Reverse Data Management (RDM) problems are those that ask: “What minimum set of interventions on a database produces a certain (desired) change in the output of a query?.” Many important problems such as deletion propagation, fairness testing, debugging, provenance factorization, causal responsibility, and resilience can be cast as RDM problems. We use a novel approach to solve RDM problems: instead of creating dedicated algorithms for easy (PTIME) and hard cases (NP-complete), we devise unified algorithms that are guaranteed to terminate in PTIME for easy cases. This approach allows us to separate the task of devising efficient algorithms and proving tractability, allowing us to solve problems “as fast as theoretically possible” and to automatically recover easy instances for generally hard problems. This leads us to both practical benefits in terms of automatic time guarantees and ease of implementation as well as new theoretical benefits such as new complexity results, and an automatic hardness gadget finder. This talk will present our unified framework and its application to the problems of resilience, causal responsibility, and minimal factorization of provenance (from work to be presented at SIGMOD 24 and PODS 24).
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Sandeep Polisetty (UMASS)*; Juelin Liu (University of Massachusetts Amherst); Hui Guan (University of Massachusetts, Amherst); Marco Serafini (University of Massachusetts Amherst)<br/>
              <strong>Split-Parallel Graph Neural Network Training</strong> <!-- <a href="placeholder.pdf">[PDF]</a> <br/> -->
              <a href="javascript: toggleVisibility ('#po13')">Toggle Abstract</a>
              <div id="po13" class="abstract" style="display: none;">
                <p>
				Graph neural networks (GNNs), an emerging class
				of machine learning models for graphs, have gained popularity
				for their superior performance in various graph analytical tasks.
				Mini-batch training is commonly used to train GNNs on large
				graphs, and data parallelism is the standard approach to scale
				mini-batch training across multiple GPUs. One of the major
				performance costs in GNN training is the loading of input features,
				which prevents GPUs from being fully utilized. In this talk, we
				argue that this problem is exacerbated by redundancies that are
				inherent to the data parallel approach. To address this issue, we
				introduce a hybrid parallel mini-batch training paradigm called
				split parallelism. Split parallelism avoids redundant data loads
				and splits the training of each mini-batch across multiple GPUs
				online, at each iteration, using a lightweight splitting algorithm.
				We implement split parallelism in a novel system named Spara
				and show that it outperforms state-of-the-art mini-batch training
				systems like DGL, Quiver, and P3 
                </p>
              </div>
          </td>
        </tr>

        </tbody>


      </table>
</div>
