<div class="content-container">
  <div class="jumbotron">
    <h1>
      Poster session<br />
      <small>North East Database Day 2025 </small><br />
      <small>Thursday January 9th, 2025</small><br />
      <small><a href="https://www.brandeis.edu/computer-science/" target="_blank">Michtom School of Computer Science</a>
        @ Brandeis University</small>
    </h1>
  </div>
  <p>
  The poster session will be held in <a href="https://www.brandeis.edu/about/visiting/map.html?bldgid=0101-1" target="_blank">Levine-Ross and Lurias</a>, located adjacent to the main conference hall, Sherman @ Brandeis University. The poster session is the main venue for discussing details. <i>All authors with an accepted talk are highly encouraged to present a poster of their talk.</i></p>

   <h3>Information for Poster Presenters</h3>
   <p>We will be able to provide poster board, easels, and mounting supplies for you.
      You will need to print your poster and bring it with you to the conference.
	  The foam boards to which the posters will be attached to are 2' x 3' and can be oriented as you see fit.
	  If you have a poster printed on a board, then it can be directly placed on the easel without using our foam board.

	  Please reach out to us if you have already printed a poster of different size or have any questions.</p>

  <p>The poster session will include coffee and light snacks.</p>


  <!--
   The poster session will be held in the 2nd floor of the <a href="https://www.bu.edu/cds-faculty/explore/bu-center-for-computing-data-sciences/" target="_blank">Center for Computing & Data Sciences (CCDS)</a> @ Boston University.
   The poster session is the main venue for discussing details. All the presenters (of long and short talks) are highly encouraged to present a poster of their talk.</p>

   <h3>Information for Poster Presenters</h3>
   <p>More details to be added soon.</p>
   <p>We will be able to provide poster board, easels, and mounting supplies for you.
      You will need to print your poster and bring it with you to the conference.
	  The foam boards to which the posters will be attached to are 2' x 3' and can be oriented as you see fit.
	  The posters can be attached to the foam boards with nails, tape or <a href="https://m.media-amazon.com/images/I/61V6Yl7BCTL.jpg" target="_blank">binder clips</a>.
	  If you have a poster printed on a board, then it can be directly placed on the easel without using our foam board.

	  Please reach out to us if you have already printed a poster of different size or have any questions.
-->

  <!--
	  We recommend
      posters to be either A0, A1, or ANSI D or E sizes (either 24" by 32" or 36" by 42"). Our
      poster boards are large enough to accommodate any of these (4' x 6'). You may orient your
      poster as you see fit. If you prefer to print out individual 8.5" x 11" pages, our board
      will be large enough to accommodate about 12 pages. If you prefer a single poster and
      you don't have access to a large format printer, Fedex/Kinkos can print your poster for you.
	  -->
  </p>




  <h3>List of accepted posters:</h3>


	<p style="text-align: center;">
	<a href="javascript: toggleVisibility ('showall')">Show</a>/<a href="javascript: toggleVisibility ('hideall')">Hide all abstracts</a>
	</p>

   <table class="table table-bordered posterTable">
      <tbody>
        <tr>
          <td align="center">list of accepted posters will be made available here soon</td>
        </tr>

         <!--tr>
          <td>
              Koyena Pal (Northeastern University)*; David Bau (Northeastern University); Ren茅e J. Miller (Northeastern University)<br/>
              <strong>Model Lakes</strong> <a href="download/posters/Model_Lakes.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po0')">Toggle Abstract</a>
              <div id="po0" class="abstract" style="display: none;">
                <p>
                Given a set of deep learning models, it can be hard to find models appropriate to a task, understand the models, and characterize how models are different one from another. Currently, practitioners rely on manually-written documentation to understand and choose models. However, not all models have complete and reliable documentation. As the number of machine learning models increases, this issue of finding, differentiating, and understanding models is becoming more crucial. Inspired from research on data lakes, we introduce and define the concept of model lakes. We discuss fundamental research challenges in the management of large models. And we discuss what principled data management techniques can be brought to bear on the study of large model management.
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Victor Giannakouris (Cornell University)*; Immanuel Trummer (Cornell University)<br/>
              <strong>-Tune: Exploiting Large Language Models for Automated Database System Tuning</strong> <a href="download/posters/lambda-Tune_Exploiting_Large_Language_Models_for_Automated_Database_System_Tuning.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po1')">Toggle Abstract</a>
              <div id="po1" class="abstract" style="display: none;">
                <p>
                We introduce 位-Tune, a framework that leverages Large Language Models (LLMs) for automated, workload-adaptive database system tuning. 位-Tune harnesses the ability of LLMs to process and comprehend arbitrary textual data in a zero-shot manner, employing a workload-adaptive optimization approach. Given a database system, the hardware specifications, and a set of queries, 位-Tune automatically generates prompts to retrieve configuration recommendations for the tuning knobs and the physical design, tailored to the specific system and workload. Our framework utilizes a workload compression approach that extracts and includes in the prompt only the most insightful workload characteristics, by extracting and compressing repetitve query components, such as joins or selections, while the prompt size can also be adjusted by a user-defined token budget. As the LLM may return multiple configuration sets, 位-Tune employs a configuration evaluation approach that incrementally tries out different configurations in a round-robin fashion, until the optimal configuration is found, ensuring that the wasted efforts on evaluating inefficient configurations are minimized. Our experimental evaluation shows that 位-Tune outperforms other LLM and machine learning-enhanced database tuning baselines in use-cases where there are no already existing indexes, while it approaches the performance of its competitors in the presence of the original indexes, both in TPC-H and Join Order Benchmark.
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Ju Hyoung Mun (Boston University)*; Jinqi Lu (Boston University); Shahin Roozkhosh (Boston University); Renato Mancuso (Boston University); Manos Athanassoulis (Boston University)<br/>
              <strong>Relational Memory Controller</strong> <a href="download/posters/Relational_Memory_Controller.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po2')">Toggle Abstract</a>
              <div id="po2" class="abstract" style="display: none;">
                <p>
                Modern database systems are designed around either a row-major or a column-major data layout. Since no layout is an absolute winner, state-of-the-art systems employ either a columnar accelerator for analytical queries or hybrid layouts that bridge the benefits of the two layouts. However, both lead to data duplication and require additional bookkeeping, increasing system complexity.
                Following recent advancements in hardware, Relational Memory (RM), a new paradigm for on-the-fly data transformation, allows the CPU to access any hybrid data layout without actually storing it in memory. This offers higher throughput and increased locality since only the desired data are pushed through the memory hierarchy. The original RM design resides on programmable logic between the CPU and the memory controller. As a result, it uses the data geometry information to issue load instructions to memory only for cache lines that contain useful data; however, unnecessary data are fetched from memory and filtered out on the fly. The current design has two limitations. First, the unnecessary data read by RM reduces the goodput (throughput of useful data). Second, it misses the opportunity to convert application semantics to increased parallelism and higher overall performance.
                This work addresses the above limitations by integrating the Relational Memory within a memory controller, termed Relational Memory Controller (RMC). To do this, we first develop a configurable memory controller, to which we then integrate RM. RMC performs the data transformation closer to the data and uses the exact access patterns to extract the best performance from memory. RMC brings two benefits. First, RMC minimizes the unnecessary data movement. Only the truly necessary data is transferred through the system hierarchy. Second, RMC utilizes semantic information to maximize memory utilization. RMC allows configuring the memory controller and arranging the access sequence to maximize memory utilization.
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Maxwell Norfolk (Penn State University)*; Jack Norfolk (Bloomsburg University); Dong Xie (Penn State University)<br/>
              <strong>FishNet: Calibrating Fluid ETL Pipelines and Ad-hoc Queries for Fresh Data Exploration</strong> <a href="download/posters/FishNet_Calibrating_Fluid_ETL_Pipelines_and_Ad-hoc_Queries_for_Fresh_Data_Exploration.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po3')">Toggle Abstract</a>
              <div id="po3" class="abstract" style="display: none;">
                <p>
				In this poster, we highlight our recent work FishNet, a system that leverages a fluid ETL pipeline to allow data preprocessing efforts to be changed on the fly as interests changes. 
				Our work consists of a Just-in-Time (JIT) compiler used to create these data preprocessing efforts, a tracking service to declare the regions of data these structures are defined on, and an automatic query planner using a pandas-like DataFrame API.
                </p>
              </div>
          </td>
        </tr>

         <tr>
          <td>
              Po Hao Chen (Brown University)*; Yuchen Lu (Boston University); Yuhang Song (Boston University); Naima Abrar Shami (Boston University); Talia Chen (Boston University); Melissa Martinez (Boston University); Vasiliki Kalavri (Boston University)<br/>
              <strong>Scaling GNN training to billion-edge graphs on a single machine</strong> <a href="download/posters/Scaling_GNN_Training_to_Billion-Edge_Graphs_on_a_Single_Machine.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po4')">Toggle Abstract</a>
              <div id="po4" class="abstract" style="display: none;">
                <p> Graph Neural Network (GNN) training algorithms commonly perform neighborhood sampling to construct fixed-size mini-batches for weight aggregation on GPUs. 
                    State-of-the-art disk-based GNN frameworks compute sampling on the CPU, transferring edge partitions from disk to memory for every mini-batch. We argue that this design incurs significant waste of PCIe bandwidth, as entire neighborhoods are transferred to main memory only to be discarded after sampling. In this paper, we make the first step towards an inherently different approach that harnesses near-storage compute technology to achieve efficient large-scale GNN training. We target a single machine with one or more SmartSSD devices and develop a high-throughput, epoch-wide sampling FPGA kernel that enables pipelining across epochs. When compared to a baseline random-access sampling kernel, our solution achieves up to 4.26 x lower sampling time per epoch.
                </p>
              </div>
          </td>
        </tr-->

  <!--		<tr>
          <td>
              Tianyu Li (MIT CSAIL)*; Badrish Chandramouli (Microsoft Research); Philip A Bernstein (Microsoft Research); Samuel Madden (MIT)<br/>
              <strong>Distributed Speculative Execution for Resilient Cloud Applications</strong> <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po5')">Toggle Abstract</a>
              <div id="po5" class="abstract" style="display: none;">
                <p>
				Distributed fault-tolerance is a critical concern for modern cloud applications built with microservices. Typical solutions incur high overhead by delegating fault-tolerance entirely to each microservice (i.e., database-per-service pattern), which forces any workload to synchronously wait for persistence for each call before proceeding to subsequent logic. We present the novel abstraction of distributed speculative execution (DSE), where the cloud runtime transparently elides synchronous persistnce and execute subsequent application logic for latency gains. In the case of failure, the runtime automatically repairs inconsistencies using a rollback-recovery scheme. The DSE runtime releases speculative results to external clients only after asynchronous persistence catches up, and ensures that clients observe indistinguishable results from non-speculative execution. We present our design and implementation of the first DSE framework, libDSE. libDSE includes implementation of rollback-recovery protocol based on the recently propsoed DPR protocol, and a programming framework to help service providers implement DSE-capable services. We assemble 3 end-to-end applications using 4 speculative services implemented with libDSE. Our evaluation shows that libDSE can reduce end-to-end latency by an order of magnitude in highly distributed cloud applications while adding minimal overhead.
                </p>
              </div>
          </td>
        </tr> -->

  <!--tr>
          <td>
              Yuanli Wang (Boston University)*; Lei Huang (Boston University); Zikun Wang (Boston University); Vasiliki Kalavri (Boston University);  Ibrahim  Matta (Boston University)<br/>
              <strong>Automatic task placement decisions for high-throughput and resource-efficient data stream processing</strong> <a href="download/posters/Automatic_task_placement_decisions_for_high-throughput_and_resource-efficient_data_stream_processing.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po6')">Toggle Abstract</a>
              <div id="po6" class="abstract" style="display: none;">
                <p>
				Automatic configuration of distributed steaming queries has been a central concern in systems research. In this work, we study task placement, a crucial aspect of streaming configuration that has so far been largely overlooked. We find that task placement has a tremendous effect on query performance and resource efficiency, yet, stream processing systems currently employ naive  random strategies. The optimal task placement problem is NP-hard, making exact solutions impractical for dynamic, long-running streaming workloads. We propose Contention-Aware Placement Search (CAPS), a placement strategy that leverages empirically-verified heuristics to prune the vast search space of alternative task assignments and efficiently find satisfactory placement plans.

                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Aaron Ang (Boston University)*; Subhadeep Sarkar (Brandeis University); Manos Athanassoulis (Boston University)<br/>
              <strong>End-to-end Persistent and Timely Deletion</strong> <a href="download/posters/End-to-end_Persistent_and_Timely_Deletion.pdf" target="_blank">[PDF]</a> <br/
              <a href="javascript: toggleVisibility ('#po7')">Toggle Abstract</a>
              <div id="po7" class="abstract" style="display: none;">
                <p>
				Today, regulations like the GDPR and CCPA aim to minimize user data exposure by delineating strict data management guidelines. Modern data systems must align with these legal frameworks designed to protect users' online exposure in an age of increasing emphasis on data security and privacy. The End-to-end Persistent and Timely Deletion project addresses this challenge by incorporating application support for Lethe, a novel Log-Structured Merge delete engine, into MySQL. Lethe provides persistence guarantees for primary delete operations by introducing a global delete persistence threshold (DPT) representing the maximum duration a key can exist after deletion. The poster extends this work by introducing techniques to support multiple arbitrary DPT values (per table or operation) issued by the client. On the application level, it introduces new keywords to MySQL and modifies the SQL parser to store and relay this information to the storage engine. On the storage engine level, it computes a time-to-live for each tombstone per level, which is then aggregated per file to determine which files to compact.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Saurabh Bajaj (University of Massachusetts, Amherst)*; Hui Guan (University of Massachusetts, Amherst); Marco Serafini (University of Massachusetts Amherst)<br/>
              <strong>Mini-Batch vs Full-Graph Showdown: GNN Training systems</strong-->
  <!-- <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/> -->
  <!--a href="javascript: toggleVisibility ('#po8')">Toggle Abstract</a>
              <div id="po8" class="abstract" style="display: none;">
                <p>
				Graph Neural Networks (GNNs) have gained significant
				attention in recent years due to their ability to learn representations of graph-structured data. Two common methods for training GNNs are mini-batch training and full-graph training. 
				Since these two methods require different training pipelines and systems optimizations, two separate categories of GNN training systems emerged, each tailored for one method only. 
				Previous research introducing systems belonging to a particular category predominantly compares them with other systems within the same category, offering limited or no comparison with systems from the other category.
				Some prior work also argues that one training method is superior to the other to justify their focus on that method, but the existing evidence is incomplete and contradictory. In this paper, we empirically compare the two training methods and GNN training systems that support them in terms of accuracy and performance.
				Our experiments demonstrate that both training techniques converge to similar accuracy values across multiple datasets and models, with a slight advantage for full-graph training.
				Mini-batch training is less sensitive to model architecture changes, making hyperparameter search easier.
				The mini-batch training systems we consider consistently converge faster than the full-graph training ones across multiple datasets, GNN models, and system configurations, between 2.4x - 15.2x faster.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Ferdinand Kossmann (MIT)*; Ziniu Wu (Massachusetts Institute of Technology); Nesime Tatbul (MIT); Lei Cao (University of Arizona/MIT); Samuel Madden (MIT)<br/>
              <strong>Optimizing model serving through conditional inference</strong> <a href="download/posters/Optimizing_Model_Serving_through_Conditional_Inference.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po9')">Toggle Abstract</a>
              <div id="po9" class="abstract" style="display: none;">
                <p>
            				Serving predictions of machine learning (ML) models is a pain point for many companies. First, ML models incur high computational costs. Second, request patterns in practice are spikey and unpredictable, which makes it hard to optimally provision hardware. In this work, we leverage a conditional inference mechanism that allows to (i) achieve prediction accuracies with less work, and (ii) adjust the computational demand of inference to the incoming workload in a fine-grained manner. While conditional inference is promising, it also comes with its own set of challenges. We therefore propose CondServe, an end-to-end serving system that leverages conditional inference and solves these challenges. In our evaluation, we show that CondServe can consistently improve the latency, accuracy and cost of inference serving over state-of-the-art baselines.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Samuel M Buxbaum (Boston University)*; Muhammad Faisal (Boston University); Eli M Baum (Boston University); Yan Tong (UC Santa Cruz); Vasiliki Kalavri (Boston University); John Liagouris (Boston University); Mayank Varia (Boston University)<br/>
              <strong>QueryShield: Cryptographically Secure Analytics in the Cloud</strong> <a href="download/posters/QueryShield_Cryptographically_Secure_Analytics_in_the_Cloud.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po10')">Toggle Abstract</a>
              <div id="po10" class="abstract" style="display: none;">
                <p>
				Secure multiparty computation (MPC) is a cryptographic technique which allows multiple distrusting parties to collaboratively compute functions of their private data without revealing any information other than the output of the computation. The need for secure collaborative analytics emerges in various scenarios, where the societal or monetary benefit is magnified if multiple -- often competing -- entities allow analyses on the union of their sensitive data.

				In this poster, we present QueryShield, a service for streamlined, cryptographically secure data analytics in the cloud. With QueryShield, data analysts can advertise analysis descriptions to data owners, who may agree to participate in a computation for profit or for the greater good, provided that their data remain private. QueryShield supports relational and time series analytics with provable data privacy guarantees using MPC. At the same time, it makes MPC accessible to non-expert users by offering a familiar web interface and fully-automated orchestration of cryptographic computations.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Franco Solleza (Brown University)*; Shihang Li (University of Washington); William H Sun (Brown University); Richard X Tang (Brown University); Malte Schwarzkopf (Brown University); Nesime Tatbul (MIT); Andrew Crotty (Northwestern University); David E Cohen; Stan Zdonik (Brown University)<br/>
              <strong>Mach: Firefighting Time-Critical Issues in Complex Systems Using High-Frequency Telemetry</strong> <a href="download/posters/Mach_Firefighting_Time-Critical_Issues_in_Complex_Systems_Using_High-Frequency_Telemetry.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po11')">Toggle Abstract</a>
              <div id="po11" class="abstract" style="display: none;">
                <p>
				To understand the complex interactions in modern software, engi- neers often rely on high-frequency telemetry (HFT) data generated via tools like eBPF. But todays database systems are too slow for HFTs rate and volume, and cannot process it within the limited resources available on host machines.
				Mach is a new storage engine for collecting and querying HFT. Key to Mach is the Temporal Skip Index (TSI)a new, lightweight indexing structure specialized to HFT. Mach supports high ingest rates and makes data immediately queryable while operating with- in a limited on-host resource envelope.
				In this poster, we show how Mach helps engineers collect and query HFT in near real-time when diagnosing a performance problem. By contrast, current systems and data reduction techniques fail to keep up. While the baseline (InfluxDB) drops much of the HFT, Mach keeps up with the load and lets the audience interactively explore HFT from application and kernel events as they arrive.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Jennifer Lee (Northeastern University)*<br/>
              <strong>SQLearn: Automating Partial Grading for SQL Queries</strong> <a href="download/posters/SQLearn_Automating_Partial_Grading_for_SQL_Queries.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po12')">Toggle Abstract</a>
              <div id="po12" class="abstract" style="display: none;">
                <p>
				SQLearn is an innovative project aimed at streamlining the grading process for SQL queries. Traditional methods of evaluating SQL assignments often involve manual inspection, which can be time-consuming and prone to errors. Our solution leverages automation to provide efficient and accurate partial grading, significantly reducing the workload for instructors and providing timely feedback to students. Through a combination of syntax analysis and semantic understanding, we evaluate queries based on similarity of clauses. 
				With SQLearn, educators can focus their attention on providing targeted guidance and support to students, fostering a more productive learning environment. By automating the grading process, SQLearn enhances efficiency, accuracy, and consistency in assessing SQL proficiency, ultimately benefiting both instructors and learners.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Jeffrey Tao (University of Pennsylvania)*; Ryan Marcus (University of Pennsylvania)<br/>
              <strong>Bayesian Query Super-Optimization</strong> <a href="download/posters/Bayesian_Query_Super-Optimization.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po13')">Toggle Abstract</a>
              <div id="po13" class="abstract" style="display: none;">
                <p>
				Analytics database workloads often contain queries that are executed repeatedly. Despite being known ahead of time, these queries are usually optimized online in the same fashion as ad-hoc queries. For such queries, techniques ought to exist to find better plans than can be found using traditional heuristic query planners that use cost and cardinality estimates, even if such techniques require longer optimization time, as the upfront investment in optimization time will pay off as the query is executed more times. We present a novel strategy for iteratively searching the space of possible query plans by performing Bayesian optimization over a latent space that encodes all possible query plans for a given schema. Encoded query plans select both join orders and join operators. As optimization proceeds, proposed plans are executed against the database to record their real execution times, and the Bayesian surrogate model makes better predictions of the execution times of latent-space plans. We introduce a novel timeout-based strategy in order to minimize the impact of bad plans on the running time of the optimization process while still gaining useful information from timed-out plans by recording censored observations. We evaluate our technique on the Join Order Benchmark and find that it successfully finds plans that are strictly better than any plans achievable by providing optimal hints to the Postgres optimizer.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Zixuan Yi (University of Pennsylvania)*; Yao Tian (The Hong Kong University of Science and Technology); Zack Ives (University of Pennsylvania); Ryan Marcus (University of Pennsylvania)<br/>
              <strong>Low Rank Approximation for Learned Query Optimization</strong> <a href="download/posters/Low_Rank_Approximation_for_Learned_Query_Optimization.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po14')">Toggle Abstract</a>
              <div id="po14" class="abstract" style="display: none;">
                <p>
				We present LimeQO, a learned steering query optimizer based on linear methods, such as matrix completion, for repetitive workloads. LimeQO can forgo expensive neural networks by taking advantage of the low-rank structure of query workloads. Using offline execution, LimeQO can accelerate workloads by up to 2x with zero regressions in just a few hours, while using 100-1000x fewer computational resources than deep learning techniques.
                </p>
              </div>
          </td>
        </tr-->

  <!--		<tr>
          <td>
              Fatemeh Nargesian (University of Rochester)*; Amir Gilad (The Hebrew University)<br/>
              <strong>Query Answering on Public Data with Private Queries</strong> <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po15')">Toggle Abstract</a>
              <div id="po15" class="abstract" style="display: none;">
                <p>
				With the emergence of open data portals and data markets, privacy protection is becoming a crucial aspect of query answering and information retrieval. While much of the data privacy research has focused on query answering on private databases, little attention has been paid to guaranteeing the privacy of the query, where a user intends to privately query a publicly available database without revealing any information about the query. The overarching goal of this project is to facilitate accurate and oblivious query answering over public data. At a high level, in this setting, a user holds an SQL query while a server holds a set of public tables. The user enters the query in a client (e.g., a web-based app) which interacts with the server to obtain results for the query without revealing the query.
				Differential Privacy (DP) has emerged as one of the leading approaches for securing data due to its strong rigorous guarantees. Commonly, in DP, the result of an algorithm over some data cannot be distinguished from the result of that algorithm over similar data, thus obfuscating the individual details in the database. However, it is not always the case that the data has to remain hidden. In our case, the database is public, but instead, the user query needs to be kept private. This means a query over some data must not be distinguishable from a similar query over the same data by investigating their results.
				In this poster, we will report on our progress to model the problem using DP. We will discuss various techniques for creating the embedding space for an SQL query and report experimental results on the accuracy of query results.
                </p>
              </div>
          </td>
        </tr>
        -->
  <!--tr>
          <td>
              Andy Huynh (Boston University); Anwesha Saha (Boston University)*; Harshal Chaudhari (Boston University); Manos Athanassoulis (Boston University)<br/>
              <strong>Axe: Learning to Tune LSM Trees with Learned Cost Models</strong> <a href="download/posters/Axe_Learning_to_Tune_LSM_Trees_with_Learned_Cost_Models.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po16')">Toggle Abstract</a>
              <div id="po16" class="abstract" style="display: none;">
                <p>
				As the complexity of modern applications continues to grow, the flexibility of Log-Structured Merge (LSM) trees is responsible for their rise in popularity as the data structure of choice for key-value stores and some database management systems. Prior work traditionally used analytical cost models to tune LSM trees given an incoming workload; however, such analytical cost models have several drawbacks. They require intimate domain knowledge to be created, do not always accurately capture the real-system behavior, especially for highly complex designs, and they may fail the requirements of off-the-shelf numerical optimizers such as continuity and differentiability. Recently, machine learning techniques aim to address these issues by circumventing a cost model through an end-to-end tuning approach that learns a cost surface for a particular workload to suggest a configuration. However, these methods are expensive because (a) they rely on real workload execution with different tunings to receive feedback, and (b) they have to balance exploration and exploitation.
				In this work, we propose Axe, an LSM tuning paradigm that addresses the issues above. Axe creates a learned cost model that is a differentiable and continuous surrogate model using data from an analytical cost model. We show that a learned cost model works well for tuning simple LSM designs using off-the-shelf optimizers. However, the optimizer fails to navigate the LCM of complex designs with categorical tuning knobs. To address this, we use the learned cost model as the loss function for another neural network that learns to navigate the cost model, which we call a learned tuner. By addressing the challenges of analytical cost models with learned cost models, we can leverage gradient-based methods on harder problems, such as tuning with uncertainty in the workload expectation or optimization for a given performance percentile.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Gerardo Vitagliano (MIT CSAIL)*; Michael Cafarella (MIT CSAIL)<br/>
              <strong>Automating multimodal data integration for physical processes</strong> <a href="download/posters/Automating_multimodal_data_integration_for_physical_processes.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po17')">Toggle Abstract</a>
              <div id="po17" class="abstract" style="display: none;">
                <p>
				Physical processes are sequences of events that happen over time.
				Typically, domain experts possess knowledge or high-quality data about some properties of the processes, and they have access to experimental data that reflect the changing states of the processes.
				A common feature of real-world data for physical processes in different domains, e.g., environment science, medicine, or engineering, is that they typically comprise different modalities, such as images, natural language, and tabular data.

				For a given process, we expect some data sources to condense high quality a priori knowledge, e.g., a map of the environmental area, a genomic sequence, or engineering blueprints.
				Other data sources reflect ``observational signals'' collected over time, and they may be incomplete, incorrect or lower-quality, e.g., sensor data, health records.
				Integrating a priori information with observational data is often necessary to validate hypotheses, analyze unforeseen circumstances, or perform decision-making.

				We aim at designing a framework to empower domain experts in the analysis of physical processes.
				The goal of the framework is to automatically design and execute multimodal data integration pipelines, decoupling modeling and testing hypothesis from wrangling and integrating raw data.

				In a first stage, we plan to use generative models such as LLMs to generate a stepwise specification of the integration pipeline. 
				We expect individual steps of the pipeline to either leverage existing tools and programming modules, or require writing custom code.
				In a fully automated scenario, we would employ LLM code generation. 
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Ouael Ben Amara (University of Michigan - Dearborn)*; Niccolo Meneghetti (University of Michigan - Dearborn); Sami Hadouaj (University of Michigan - Dearborn)<br/>
              <strong>StarfishDB: a Query Execution Engine for Relational Probabilistic Programming</strong-->
  <!-- <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/> -->
  <!--a href="javascript: toggleVisibility ('#po18')">Toggle Abstract</a>
              <div id="po18" class="abstract" style="display: none;">
                <p>
				We would like to join the poster session of NEDBD24 to present our latest paper StarfishDB:
				a Query Execution Engine for Relational Probabilistic Programming [1], recently accepted for
				publication at SIGMOD 2024. The paper describes a novel probabilistic programming system, that
				uses Probabilistic Programming Datalog [2] to encode the programs and Gibbs sampling for fast,
				approximate inference. The paper extends previous results from our lab [3], introducing new features
				like recursion, advanced factorization, and just-in-time (JIT) compilation.
				The poster will report our experiences in designing and implementing StarfishDB, and the results
				of our benchmarks across various Bayesian models, including Latent Dirichlet Allocation (LDA) [4],
				the Ising/Potts model [5], and Hidden Markov Models [6]. We believe that our work is very relevant
				for the audience of NEDBD24 and we are excited and thankful for this opportunity.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Anna Zeng (MIT)*; Michael Cafarella (MIT CSAIL)<br/>
              <strong>Digging Up Threats to Validity: A Data Discovery Approach to Sensitivity Analysis</strong-->
  <!-- <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/> -->
  <!--a href="javascript: toggleVisibility ('#po19')">Toggle Abstract</a>
              <div id="po19" class="abstract" style="display: none;">
                <p>
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Huawei Lin (Rochester Institute of Technology); Weijie Zhao (Rochester Institute of Technology)*<br/>
              <strong>Toward Explainable Large Language Models via Influence Estimation</strong> <a href="download/posters/Toward_Explainable_Large_Language_Models_via_Influence_Estimation.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po20')">Toggle Abstract</a>
              <div id="po20" class="abstract" style="display: none;">
                <p>
				Recently, large language models (LLMs) have been widely used in the real world for various applications, such as translation, content generation, code development, question answering, and so forth. These powerful LLMs, trained on vast amounts of textual data, are transforming how we interact with technology and automating many language-related tasks. However, LLMs are still black-box models, and still unexplainable.

				For a given generation, can we know which training data samples have the most influence on this generation? In this poster, we present an influence estimation method for LLMs, applying first-order gradients, to estimate the influence of each training data sample on a test generation. We propose a collection of techniques to compress and speed up the influence estimation, enabling the compression of a ~26GB gradient vector of the llama-2 7b model into 125KB and maintaining the comparable estimating performance. In our experiment, our method can estimate the influence of each training data sample for 52K alpaca dataset in 7 seconds. Our empirical experiments confirm the efficiency and efficacy of our method.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Sakshi Sharma (Boston University)*; Konstantinos Kanellis (University of Wisconsin, Madison); Andy Huynh (Boston University); Manos Athanassoulis (Boston University)<br/>
              <strong>Dynamic Parameter Tuning for LSM based Databases</strong> <a href="download/posters/Dynamic_Parameter_Tuning_for_LSM_based_Databases.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po21')">Toggle Abstract</a>
              <div id="po21" class="abstract" style="display: none;">
                <p>
				Log-Structured Merge (LSM) trees are a popular choice of data structure for key-value database systems due to their high ingestion rate and fast reads. They achieve this by appending new writes and updates sequentially and buffering changes in memory before flushing them to disk in sorted order. The LSM tree behavior can be dynamically altered by a large set of tunable parameters to accomodate a wide range of workloads. While these parameters provide flexibility, identifying the optimal value for these parameters, in order to maximize system performance, is a known hard
				problem. Offline tuning approaches can provide optimal configurations, however, they require knowledge about the workload apriori and/or evaluating hundreds of configurations, meaning that they lack the flexibility to adapt to evolving conditions. In the online setting, evaluating the
				performance impact of a particular tuning knob can be an expensive endeavor.
				To this end, we propose Onix, a tuning framework that focuses on tackling the online setting of the tuning problem, specifically dynamically tuning LSM trees using Bayesian Optimization (BO). BO constructs a probabilistic model to navigate the space of tuning knobs, striking a careful balance between exploring uncharted parameter configurations and exploiting areas already identified as promising. We leverage BOs efficient convergence to minimize the number of configurations deployed for exploration. Onix integrates Microsofts BO-based system tuning framework, MLOS, with Metas LSM tree implementation, RocksDB. As workloads are executed on RocksDB, Onix propages appropriate information to MLOS, which in turn recommends the correct configuration for the current workload. In the best-case scenario, Onix can achieve up to 2 better performance (in terms of average read latency) than the default configuration, while performing at least as well as default tuning in the worst case guaranteeing no performance regression.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Alexander H Ott (Brandeis University )*; Arpita Saha (Brandeis University); Subhadeep Sarkar (Brandeis University)<br/>
              <strong>Towards Workload-Aware Self-Designing LSM Engines</strong> <a href="download/posters/Towards_Workload-Aware_Self-Designing_LSM_Engines.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po22')">Toggle Abstract</a>
              <div id="po22" class="abstract" style="display: none;">
                <p>
				Finding the optimal system tuning to achieve the best performance is an open research problem in databases. 
				With workload properties changing frequently and system design becoming increasingly complex, finding the best tuning for a given workload is incredibly hard. We propose to build a self-designing log-structured merge (LSM)-based data store that can transition between designs on the fly subject to workload changes. Its machine learning core learns from large-scale experimental data to identify (i) the most important tuning knobs, (ii) the interplay among the knobs, and (iii) the best storage engine configurations for any workload composition and distribution. Finally, based on these recommended settings, the templated storage engine initiates a lazy transition toward the optimal design for the current workload while maintaining stable performance.  
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Bruno Scarone (Northeastern University)*<br/>
              <strong>A Principled Approach to Measure Data Bias</strong> <a href="download/posters/A_Principled_Approach_to_Measure_Data_Bias.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po23')">Toggle Abstract</a>
              <div id="po23" class="abstract" style="display: none;">
                <p>
				We propose a novel perspective on how to define bias based on an algorithmic specification and develop a principled approach that allows bias to be analytically quantified and mitigated.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Teona Bagashvili (Boston University)*; Tarikul Islam Papon (Boston University); Manos Athanassoulis (Boston University)<br/>
              <strong>ZNS SSDs: Architectural Insights and Performance Analysis</strong> <a href="download/posters/ZNS_SSDs_Architectural_Insights_and_Performance_Analysis.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po24')">Toggle Abstract</a>
              <div id="po24" class="abstract" style="display: none;">
                <p>
				Zoned Namespace (ZNS) SSDs represent a paradigm shift in flash-based drives, transferring the responsibility of space management from the device to the host. This leads to benefits such as lower write amplification, high data placement flexibility and reduced overhead of garbage collection. ZNS provides these benefits by introducing the concept of an SSD zone, where data must be written sequentially. This new interface opens up new opportunities and challenges for the storage stack, requiring a deeper understanding of the characteristics of ZNS SSDs. In this  study we provide a comprehensive overview of zoned storage principles and an analysis of ZNS SSDs performance. We evaluate a ZNS SSD device when executing workloads with sequential or random I/Os, and a mixed read-write workload. We also explore the impact of zone state transitions, concurrent I/O operations, and the effect of the limit on the maximum open zones on performance. We observe that the interference of the zone reset command with sequential writes is not negligible. We further notice that the device achieves a higher concurrency level within the zone rather than across zones. Overall, we conclude that identifying similar properties and performing a thorough analysis of ZNS SSDs is a fundamental step towards integrating zoned storage into current systems and designing new applications.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Shubham Kaushik (Brandeis University)*; Manos Athanassoulis (Boston University); Subhadeep Sarkar (Brandeis University)<br/>
              <strong>RangeReduce: A Range Query Driven Compaction for LSM-Trees</strong> <a href="download/posters/RangeReduce_A_Range_Query_Driven_Compaction_for_LSM-Trees.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po25')">Toggle Abstract</a>
              <div id="po25" class="abstract" style="display: none;">
                <p>
				Log-structured merge (LSM) trees are widely used in modern key-value stores to provide high write throughput and competitive query performance. However, this superior write performance comes at the cost of high write amplification and increased cost of range queries. LSM-trees realize updates and deletes logically by inserting newer versions of the data into the tree which results in high space amplification. Thus, when processing update and delete-intensive workloads, state-of-the-art LSM-based storage engines end up reading large chunks of logically invalidated data, and this leads to poor range query throughput. In addition, the logically invalidated data are also re-written to the disk several times during compactions, resulting in high write amplification. In this work, we introduce a new family of compaction strategies that takes advantage of the reads performed during range queries to compact the data and write them back as new files with fewer invalid data. The objective is to piggyback on the range queries to reduce the space amplification, which in turn, would reduce write amplification and improve range query performance.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Aneesh Raman (Boston University)*; Andy Huynh (Boston University); Jinqi Lu (Boston University); Ryan Marcus (University of Pennsylvania); Manos Athanassoulis (Boston University)<br/>
              <strong>Learned Indexes and Data Sortedness: A Case Study</strong> <a href="download/posters/Learned_Indexes_and_Data_Sortedness_A_Case_Study.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po26')">Toggle Abstract</a>
              <div id="po26" class="abstract" style="display: none;">
                <p> Database systems use indexes on frequently accessed attributes to accelerate query and transaction processing. This requires paying the cost of maintaining and updating those indexes, which can be thought of as the process of adding structure (e.g., sort) to an otherwise unstructured data collection. The indexing cost is redundant when data arrives pre-structured, i.e., pre-sorted, even if only up to some degree. While recent work has studied how classic indexes like B+-trees cannot fully exploit the intrinsic data near-sortedness during ingestion, there is a lack of this exploration on index designs like read-optimized learned indexes or write-optimized LSM-trees.
                    <br><br>In this project, we bridge this gap by conducting the first-ever study on the behavior of state-of-the-art learned indexes and LSM-trees when varying the degree of data sortedness in an ingestion workload. Specifically, we build on prior work on benchmarking data sortedness on B+-trees and we expand the scope to benchmark: (i) ALEX and LIPP, which are updatable learned index designs; and (ii) the LSM-tree engine offered by RocksDB. We present in detail, our evaluation framework and present key insights on the performance of learned indexes and LSM-tree designs with respect to data sortedness. Our observations indicate that learned indexes exhibit unpredictable performance when ingesting differently sorted data, while LSM-trees can benefit from sortedness-aware optimizations. We highlight the potential headroom for improvement and lay the groundwork for further research in this domain.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Ning Wang (Cornell University)*; Amir Gilad (The Hebrew University); Sainyam Galhotra (Cornell University)<br/>
              <strong>Answering What-if queries using a Probabilistic Causal Approach</strong> <a href="download/posters/Answering_What-if_queries_using_a_Probabilistic_Causal_Approach.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po27')">Toggle Abstract</a>
              <div id="po27" class="abstract" style="display: none;">
                <p>
				What-if analysis provides valuable insights for users who want to explore hypothetical scenarios without altering their databases, aiding in strategic planning across various fields. These analyses are crucial for assessing the implications of policy decisions, real-world interventions, and conducting exploratory data analysis. Typically, such analyses involve testing the effects of hypothetical updates on specific database views. However, in real-world scenarios, updates to one part of the database can impact other parts due to implicit semantic dependencies. While there is a large body of work focusing on answering aggregate what-if queries, to the best of our knowledge, no prior work has provided solutions for hypothetical ranking queries.
				To address this challenge, we introduce Inter-topK, a framework that supports top-k queries under hypothetical updates to the database. Inspired by prior work, we capture such updates with a relational causal model and consider updates as causal interventions. Due to the relative nature of top-k queries, answering them requires unraveling the dependencies between attributes and tuples even when the database is tuple-independent, necessitating a new approach. In this work, we prove that hypothetical top-k queries can be accurately answered in O(nk) time for general scenarios (where n is the database size), which can be further optimized for block-independent databases and reduced to linear time using a greedy heuristic. We theoretically prove that all our techniques yield optimal solutions for tuple-independent databases. We provide a thorough empirical evaluation, showing the quality and scalability of our solution, using three real-world datasets and two synthetic datasets. We further demonstrate how our approach can capture implicit hypothetical updates and reliably translate them to alterations in the ranking of the query.

                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Sara Alam (Tufts University)*; Johes Bater (Tufts University); Chenghong Wang (Indiana University Bloomington)<br/>
              <strong>Differentially Private Query Optimization</strong> <a href="download/posters/Differentially_Private_Query_Optimization.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po28')">Toggle Abstract</a>
              <div id="po28" class="abstract" style="display: none;">
                <p>
				Relational database systems require query optimizers in order to identify fast query plans. This ensures that queries can be answered in a timely manner, especially when they require joining multiple tables together. Optimizers rely on information stored in the system catalog, which includes statistics about the underlying data distribution. For privacy-preserving database systems, which support query execution by an untrusted server, these statistics cannot be used due to privacy concerns, which can dramatically reduce performance. In this work, we propose a privacy-preserving optimization strategy that utilizes differential privacy to release noisy statistics for use by the query optimizer. Our preliminary results show that the addition of a differentially privacy query optimization strategy does not significantly degrade system performance, allowing the system to optimize both privacy and performance.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Konstantinos Karatsenidis (Boston University)*; Aneesh Raman (Boston University); Shaolin Xie (Boston University); Matthaios Olma (MongoDB); Subhadeep Sarkar (Brandeis University); Manos Athanassoulis (Boston University)<br/>
              <strong>Concurrency for Sortedness-Aware Indexes</strong> <a href="download/posters/Concurrency_for_Sortedness-Aware_Indexes.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po29')">Toggle Abstract</a>
              <div id="po29" class="abstract" style="display: none;">
                <p>
				Indexes are vital for efficient querying in databases, but their construction and maintenance come with costs. Quick Insertion Tree (QuIT) is a B+tree-based index that excels with near-sorted data, keeping track of the hot leaf node for fast insertions. While it integrates well with concurrency control mechanisms, near-sorted data introduce challenges, like high contention for the same leaf. QuIT addresses this by relaxing order constraints, by adopting an append-only strategy until the leaf is full. This minimizes lock contention, facilitating faster insertions and concurrent operations.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Konstantinos Kanellis (University of Wisconsin-Madison)*; Badrish Chandramouli (Microsoft Research); Shivaram Venkataraman (University of Wisconsin, Madison)<br/>
              <strong>F2: Designing a Key-Value Store for Large Skewed Workloads</strong> <a href="download/posters/F2_Designing_a_Key-Value_Store_for_Large_Skewed_Workloads.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po30')">Toggle Abstract</a>
              <div id="po30" class="abstract" style="display: none;">
                <p> Today's key-value stores are either disk-optimized, focusing on large data and saturating device IOPS, or memory-optimized, focusing on high throughput with linear thread scaling assuming plenty of main memory. However, many practical workloads demand high performance for read and write working sets that are much larger than main memory, over a total data size that is even larger. They require judicious use of memory and disk, and today's systems do not handle such workloads well. We present F2, a new key-value store design based on compartmentalization. It consists of five key components that work together in well-defined ways to achieve high throughput, saturating disk and memory bandwidths, while incurring low disk read and write amplification. A key design characteristic of F2 is that it separates the management of hot and cold data, across the read and write domains, and adapts the use of memory to optimize each case. Through a sequence of new latch-free system constructs, F2 solves the key challenge of maintaining high throughput with linear thread scalability in such a compartmentalized design. Detailed experiments on benchmark data validate our design's superiority, in terms of throughput, over state-of-the-art key-value stores, when the available memory resources are scarce.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Ran Wei (Boston University)*; Zichen Zhu (Boston University); Manos Athanassoulis (Boston University)<br/>
              <strong>Benchmarking, Analyzing, and Optimizing Partial Compaction in LSM-Trees</strong> <a href="download/posters/Benchmarking_Analyzing_and_Optimizing_Partial_Compaction_in_LSM-Trees.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po31')">Toggle Abstract</a>
              <div id="po31" class="abstract" style="display: none;">
                <p>
				Log-structure merge (LSM) trees are widely adopted by many NoSQL databases because they offer high ingestion throughput via logging incoming data in a memory buffer, and flushing it to disk as a sorted run when the buffer fills up. To reduce space amplification and facilitate queries, LSM-trees periodically merge-sort data on storage to form larger sorted runs (this process is also termed as compaction). In commercial LSM-based systems, sorted runs are frequently broken down to a set of small files, allowing partial compaction (i.e., a single file is picked to compact), which effectively reduces the worst-case compaction latency without increasing the amortized write cost. However, as multiple files may exist in a sorted run, deciding which file that should be compacted affects the overall write cost, and the optimal decision remains an open research question.
				In this work, we focus on how to identify the best file to compact in an offline setting, and how different file picking policies perform for different workloads. We first design an algorithm that enumerates all possible file selection with pruning to find the minimum number of written bytes for a given workload. We then benchmark four state-of-the-art file picking policies, MinOverlappingRatio(MOR), RoundRobin(RR), OldestLargestSeqFirst(OLSF), and kOldestSmallestSeqFirst (OSSF) with different ingestion workloads, and compare them with the optimal to investigate the improvement headroom. We first observe that the headroom varies from 1.5% to 14.8%. Besides, SOTA policies performance is unstable (i.e., the number of written bytes has a significant deviation when running the same type of workload multiple times). We further study the instability of MOR, and find that the dynamic size of SSTables is the main reason.In experiments with different update distributions, MOR and RR achieve better performance, up to 17.2%. In scalability experiments, MOR outperforms other policies in the number of written bytes.

                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Shreyas Fadnavis (Janssen R&D)*; Jeffrey Fried (InterSystems); Petros Drineas (Purdue University)<br/>
              <strong>FastRet: Matrix-Sketching for Fast Vector Retrieval</strong-->
  <!-- <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/> -->
  <!--a href="javascript: toggleVisibility ('#po32')">Toggle Abstract</a>
              <div id="po32" class="abstract" style="display: none;">
                <p>
				Efficient retrieval of relevant information from large-scale vector databases is a cornerstone of modern data processing tasks. We propose the FastRet framework, a novel approach that capitalizes on the semantic proficiency of Large Language Models (LLMs) coupled with state-of-the-art sketching methods to accelerate information retrieval. Our method seamlessly processes a corpus of documents into fine-grained embeddings, subsequently applying matrix sketching techniques such as Count-sketch, Gaussian, and Sampling-based sketching. These techniques are carefully chosen for their ability to maintain the structural integrity of the data while significantly reducing the computational burden. We employ cosine similarity to measure relevance between query vectors and document embeddings, optimizing for directional alignment to ensure strong semantic correlation. Our results indicate that matrix sketching, applied to condense embeddings, can achieve consistent similarity scores using the sentence-textual-similarity benchmark. Specifically, we demonstrate that reducing a 384-dimensional embedding to 50 dimensions preserves similarity scores. Through a small-scale LLM, we validate these findings and illustrate the significant potential of this approach to reduce memory and speed overhead in retrieval tasks.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Riddho Ridwanul Haque (University of Massachusetts Amherst)*; Anh Le Xuan Mai (New York University Abu Dhabi); Azza Abouzied (New York University Abu Dhabi); Matteo Brucato (Microsoft Research); Peter Haas (University of Massachusetts Amherst); Alexandra Meliou (University of Massachusetts Amherst)<br/>
              <strong>Accelerating Stochastic Package Queries</strong> <a href="download/posters/Accelerating_Stochastic_Package_Queries.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po33')">Toggle Abstract</a>
              <div id="po33" class="abstract" style="display: none;">
                <p>
				Decision-making under uncertainty often involves making choices that limit risks while maximizing expected rewards. This in turn often translates to choosing optimal packages (multisets of tuples) from relations in probabilistic databases. For instance, during portfolio optimization, the resulting packages represent portfolios that maximize expected profits while complying with user-specified budgetary constraints and risk tolerances. A typical approach for dealing with uncertainty is to generate numerous sample realizations of the probabilistic relation, and perform constrained optimization over the samples. However, such constrained optimization problems often involve extremely large volumes of data as the input data grows both with the number of stochastic tuples in the relation and the number of samples needed to effectively represent the stochastic data. The existing SummarySearch algorithm for solving Stochastic Package Queries (SPQs) can handle large numbers of samples but struggles to deal with large numbers of tuples. We make SPQ processing scale to relations with millions of tuples, via two novel techniques: CVarification and Stochastic SketchRefine.
				CVarification rethinks existing SPQ processing approaches by providing a novel scheme to convert Value-at-Risk (VaR) constraints in SPQs to Conditional-Value-at-Risk (CVaR) constraints. This reformulation makes the size of the optimization problem independent of the number of samples used to approximate the stochastic data. Stochastic SketchRefine uses a divide-and-conquer approach that breaks down an SPQ over a relation containing millions of tuples to a sequence of more manageable SPQs over smaller subsets of these tuples, each of which is solved using CVarification combined with an off-the-shelf integer linear program solver. In our initial experiments, Stochastic SketchRefine produced packages that were almost as optimal as those created by SummarySearch, but in significantly less time.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Lina Qiu (Boston University)*; Rebecca Taft (Cockroach Labs); Alexander Shraer (Cockroach Labs); George Kollios (Boston University)<br/>
              <strong>The Price of Privacy: A Performance Study of Confidential Virtual Machines for Database Systems</strong> <a href="download/posters/The_Price_of_Privacy_A_Performance_Study_of_Confidential_Virtual_Machines_for_Database_Systems.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po34')">Toggle Abstract</a>
              <div id="po34" class="abstract" style="display: none;">
                <p>
				Confidential virtual machines (CVM) use trusted hardware to encrypt data being processed in memory to prevent unauthorized access. Applications can be migrated to CVM without changes, i.e., lift and shift, to handle sensitive workloads securely in public clouds. AMD Secure Encrypted Virtualization (SEV) is one of the prominent technologies that provides hardware support for CVM. In this work, we investigate various system operations, including CPU, memory, and disk and network I/O, to understand the performance overheads of SEV-supported CVMs. Our findings indicate that memory and I/O-intensive workloads can incur significant overhead. We then study the performance implications of running unmodified database applications, specifically CockroachDB, on CVMs by examining typical data access patterns of OLTP and OLAP workloads. A notable performance overhead of up to 18% is observed for TPC-C workload running on multinode database clusters, and an overhead of up to 13% is observed for TPC-H workload running on single-node database instances. The non-negligible overhead suggests the potential and necessity for database optimizations with respect to CVM, particularly for time-sensitive workloads. We offer a glimpse of the effect that CVM overhead can have in query planning using a simple join query: the optimal join algorithm becomes suboptimal on CVM, along with discussion of potential optimizations for reducing CVM overhead in the realm of database applications.
                </p>
              </div>
          </td>
        </tr-->

  <!--tr>
          <td>
              Carmen-Gabriela Stefanita (IBM)*<br/>
              <strong>Securing Data for AI Systems in the Era of Large Language Models</strong> <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po35')">Toggle Abstract</a>
              <div id="po35" class="abstract" style="display: none;">
                <p>
				Artificial Intelligence (AI) systems can have profound impact on individuals, societies, and economies, so there is a need for safe AI development and use following guidelines and regulations that prevent harm, discrimination, privacy violations, and other negative consequences. Data Security is under particular scrutiny because you do not need to be a security practitioner to grasp and appreciate the risks associated with data breaches -- confidential, private, or client data leaks. When building a new pipeline of AI technology, there is a data collection and handling phase where many people need access: data scientists, engineers, developers, etc. That sensitive data is a blinking red target that attackers are trying to get access to. Also, within the model development phase, there are new vulnerabilities that become new entry points that attackers will try to exploit. Open-source sharing repositories often lack comprehensive security controls, and they ultimately pass the risk onto the organization - and attackers are counting on it. In this talk, we want to show you how we are addressing data security concerns when AI is embedded into applications. We follow a holistic approach to security, by implementing an AI Risk Management framework and putting data security at the forefront of AI security. We define a Matrix of Security Controls following the MLOps (and nowadays LLMOps) process using a tool-based approach to measure the effectiveness of the implemented controls. The output metrics determine the level of success for a control. Next, we check if the outcome of those controls is compliant with regulations. Lastly, we continue iterating on this process of constant aligning with our governance objectives until we minimize security risks. As AI technologies become increasingly integrated into business applications, their vulnerabilities become more apparent, necessitating planned, robust measures to protect against potential threats. Based on practical experience.
                </p>
              </div>
          </td>
        </tr-->

  <!--tr>
          <td>
              Gerald  White (NJIT); Senjuti Basu Roy (NJIT)*<br/>
              <strong>Predict Days of Maintenance Delay (DoMD) of US Naval Vessels: Lessons Learned in Designing an Automated Data Science Pipeline using Short and Wide Obfuscated Navy Data</strong> <a href="download/posters/Predict_Days_of_Maintenance_Delay_(DoMD)_of_US_Naval_Vessels_Lessons_Learned_in_Designing_an_Automated_Data_Science_Pipeline_using_Short_and_Wide_Obfuscated_Navy_Data.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po36')">Toggle Abstract</a>
              <div id="po36" class="abstract" style="display: none;">
                <p>Periodic maintenance of naval vessels are conducted in the shipyards. 2-3 years prior to actual maintenance, it goes through extensive planning to design the optimum maintenance schedule. Yet, the US naval  vessels are prone to require extension during maintenance due to a multitude of interdependent and unforeseen factors -including inadequate planning of resources, quantity of overtime labor, direct yard cost, etc. This project develops a data science pipeline to quantitatively and accurately predict DoMD of those vessels during different phases of planning and maintenance. Data comes from the Navy Maintenance Database containing planning and execution phases of prior maintenance. It contains static and time dependent structured attributes and textual data. It is highly sparse, sources are sampled at different rates (daily, weekly, monthly, or as needed), and obfuscated as it contains controlled unclassified information. The data includes a timestamped history of several thousands of planning and execution metrics (as structured attributes) from each maintenance period or availability of a navy ship. However, there are only approximately 200 maintenance instances available for study. Designing each step of the data science pipeline thus requires special attention over this small yet highly wide obfuscated data with both static and time dependent attributes that are sampled at different rates. The developed pipeline automates data engineering efforts, including  data quality assessment, exploration, and cleansing, feature selection and engineering, hyperparameter tuning, and training an ensemble of supervised ML models that were evaluated along the way. The models are being deployed in a monthly sprint inside the Navy environment.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Grace Fan (Northeastern University)*<br/>
              <strong>Gen-T: Table Reclamation in Data Lakes</strong-->
  <!-- <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/> -->
  <!--a href="javascript: toggleVisibility ('#po37')">Toggle Abstract</a>
              <div id="po37" class="abstract" style="display: none;">
                <p>We introduce the problem of Table Reclamation. Given a Source Table and a large table repository, reclamation finds a set of tables that, when integrated, reproduce the source table as close as possible. Unlike query discovery problems like Query-by-Example or by-Target, Table Reclamation focuses on reclaiming the data in the Source Table as fully as possible using real tables that may be incomplete or inconsistent. To do this, we define a new measure of table similarity, called error-aware instance similarity, to measure how close a reclaimed table is to a Source Table, a measure grounded in instance similarity used in data exchange. Our search covers not only SELECT-PROJECT- JOIN queries, but integration queries with unions, outerjoins, and the unary operators subsumption and complementation that have been shown to be important in data integration and fusion. Using reclamation, a data scientist can understand if any tables in a repository can be used to exactly reclaim a tuple in the Source. If not, one can understand if this is due to differences in values or to incompleteness in the data. Our solution, Gen- T, performs table discovery to retrieve a set of candidates tables from the table repository, filters these down to a set of originating tables, then integrates these tables to reclaim the Source as close as possible. We show that our solution, while approximate, is accurate, efficient and scalable in the size of the table repository with experiments on real data lakes containing up to 15K tables, where the average number of tuples varies from small (web tables) to extremely large (open data tables) up to 1M tuples.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Markos Markakis (Massachusetts Institute of Technology)*; Ziyu Zhang (MIT); Rana Shahout (Harvard); Trinity Gao (MIT); Chunwei Liu (MIT); Ibrahim Sabek (University of Southern California); Michael Cafarella (MIT CSAIL)<br/>
              <strong>Press ECCS to Doubt (Your Causal Graph)</strong> <a href="download/posters/Press_ECCS_to_Doubt_Your_Causal_Graph.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po38')">Toggle Abstract</a>
              <div id="po38" class="abstract" style="display: none;">
                <p>
				Techniques from the theory of causality have seen extensive use in natural and social sciences, since they allow scientists to explicitly model assumptions about their problem and inform their conclusions accordingly. More recently, causality has also gathered interest in many computer science sub-fields, from machine learning to networks. Causal reasoning involves a causal model, most often represented as a causal graph, over the variables in the problem. When there are many variables, these graphs are often automatically generated from observational data. However, inherent biases from observational research introduce potential inaccuracies and prescribe the need for additional assumptions. Expensive manual graph verification is therefore often needed. Understanding which parts of a causal graph have the largest impact on downstream results can expedite this process and let modelers and researchers selectively validate and refine their graphs. In this work, we present ECCS, a framework for Exposing Critical Causal Structures within a causal graph, from the viewpoint of a downstream Average Treatment Effect (ATE) calculation. We formalize the problem that ECCS aims to address and present three algorithmic approaches, together with a preliminary evaluation.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Matthew D Russo (MIT)*; Chunwei Liu (MIT); Mike Cafarella (MIT); Lei Cao (University of Arizona); Peter Chen (Massachusetts Institute of Technology); Zui Chen (Tsinghua University); Tim Kraska (MIT); Samuel Madden (MIT); Gerardo Vitagliano (MIT)<br/>
              <strong>A Declarative System for Optimizing AI Workloads</strong> <a href="download/posters/A_Declarative_System_for_Optimizing_AI_Workloads.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po39')">Toggle Abstract</a>
              <div id="po39" class="abstract" style="display: none;">
                <p>
				        Modern AI models provide the key to a long-standing dream: processing analytical queries about almost any kind of data. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or facts from video streams. Today's models can often accomplish these tasks with high accuracy. However, a programmer who wants to actually answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the fastest and most accurate prompt formulation, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape changes. In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system then uses its cost optimization framework --- which explores the search space of AI models, prompting techniques, and related foundation model optimizations --- to implement the query with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself. We evaluate Palimpzest on tasks in Real Estate Search, Legal Discovery, and Medical Schema Matching. We show that even the simple Palimpzest prototype offers a range of plans that are more appealing than conventional baseline approaches, including some that offer quality equal to an all-GPT4 approach, with roughly 1/3rd the runtime and less than 1/6th the financial cost. These require no additional work by the user.
                </p>
              </div>
          </td>
        </tr>

		<tr>
          <td>
              Zixuan Chen (Northeastern University)*; Panagiotis Manolios (Northeastern University); Mirek Riedewald (Northeastern University)<br/>
              <strong>Why Not Yet: Flexible and Scalable Techniques for Explaining Rankings</strong> <a href="download/posters/Why_Not_Yet_Flexible_and_Scalable_Techniques_for_Explaining_Rankings.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po40')">Toggle Abstract</a>
              <div id="po40" class="abstract" style="display: none;">
                <p> Rankings and top- queries are ubiquitous: booking applications rank the hotels and show the top ones to users; online stores recommend products to users based on their preferences; universities admit top students. We study possible explanations for questions such as why have certain tuples not shown up in the top- result yet锟?and why are the tuples ranked in this order?锟?The former represents what we refer to as a why-not-yet problem. The latter corresponds to a ranking explanation problem. Our work focuses on the common scenario where entities are sorted based on a linear combination of scoring attributes. Our techniques cover a spectrum of problems from finding a linear ranking function that places certain tuples of interest into top- to finding a linear ranking function that best approximates a given top- ranking.
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Maureen E Kraft (IBM Data & AI)*<br/>
              <strong>Open Data Lakehouse</strong--> <!-- <a href="placeholder.pdf" target="_blank">[PDF]</a> <br/> -->
  <!--a href="javascript: toggleVisibility ('#po41')">Toggle Abstract</a>
              <div id="po41" class="abstract" style="display: none;">
                <p>
				Data is everywhere. Its the fabric of our lives. You find structured data in retail and banking and unstructured data in such things as images, social media, and emails. Today the open data lake house brings together structured and unstructured data to derive insights, build chat bots and leverage generative AI Large Language Models. IBM recently delivered watsonx.data, an open data lake house solution thats open, flexible, and cost effective. Joining forces with our business partner, Cloudera, IBM is focusing on providing our clients solutions that leverage their data without need to move or replicate their data nor depend on ETL.

				Data is everywhere. Its the fabric of our lives. You find structured data in retail and banking and unstructured data in such things as images, social media, and emails. Today the open data lake house brings together structured and Cloudera manages over 25 exabytes of data. Thats a lot of data and would really be expensive to try to move it. Leveraging open source such as Apache Iceberg table format, and open query engines such as Presto and Spark, IBM and Cloudera are providing our customers to access all their big data, generating modern applications and analysis to help them tackle some of todays real-world problems. 

				Data is everywhere. Its the fabric of our lives. You find structured data in retail and banking and unstructured data in such things as images, social media, and emails. Today the open data lake house brings together structured and In this talk, I will introduce the concept of the open data lakehouse and walk the audience through a use case of how generative AI can be leveraged on data across the lakehouse to solve real-world problems such as interpreting insurance claim data or summarizing text.  
                </p>
              </div>
          </td>
        </tr>
		<tr>
          <td>
              Cagatay Demiralp (MIT CSAIL)*;  Fabian  Wenz (MIT); Peter Chen (Massachusetts Institute of Technology); Moe Kayali  (University of Washington); Nesime Tatbul (MIT); Michael Stonebraker (MIT)<br/>
              <strong>Making LLMs Work for Enterprise Data Tasks</strong> <a href="download/posters/Making_LLMs_Work_for_Enterprise_Data_Tasks.pdf" target="_blank">[PDF]</a> <br/>
              <a href="javascript: toggleVisibility ('#po44')">Toggle Abstract</a>
              <div id="po44" class="abstract" style="display: none;">
                <p>
				Large language models (LLMs) have shown strong performances on natural language comprehension tasks, from summarization to question answering. The power of these models comes from optimizing for simple self-supervised learning tasks such as next token prediction using massive public web texts as training data on a scalable and adaptive architecture. However, by construction, LLMs know little about enterprise database tables in the private data ecosystem, which differ substantially from web text in structure and content. Given LLMs performance is tied to their training data,  a crucial question is how useful they can be in improving enterprise database management and analysis tasks.  To help contend with this question, we contribute (1) preliminary experimental results on the performance of LLMs for text-to-SQL and semantic column-type detection tasks on enterprise datasets and (2) a discussion of challenges and potential solutions for effectively utilizing LLMs in enterprise settings. 
                </p>
              </div>
          </td>
        </tr-->

        </tbody>


      </table>
</div>