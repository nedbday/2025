<div id="program" class="content-container">
<div class="jumbotron">
  <h1>
     Program<br />
     <small>North East Database Day 2024 </small><br />
     <small>Thursday May 23rd, 2024</small><br/>
     <small><a href="https://www.bu.edu/cpo/project/center-for-computing-data-sciences/" target="_blank">Center for Computing & Data Sciences (CCDS)</a> 1750 @ Boston University</small>
  </h1>
</div>
      <!--h2>Special thanks to this year’s sponsors:</h2-->
      <h3>Coming soon!</h3>
      <!--p style="text-align: center;">
		  <a href="https://www.bu.edu/" target="_blank"><img src="images/logos/bu_all.png" style="height: 100%; max-height: 65px; width: auto;" /></a> &nbsp; &nbsp;
		  <a href="https://www.influxdata.com/" target="_blank"><img src="images/logos/influxdb.svg" style="height: 100%; max-height: 65px; width: auto;" /></a> &nbsp; &nbsp;
		  <br/><br/>
      <a href="https://tiledb.com/" target="_blank"><img src="images/logos/tiledb.png" style="height: 100%; max-height: 65px; width: auto;" /></a> &nbsp; &nbsp;
      <a href="https://www.mongodb.com/" target="_blank"><img src="images/logos/MongoDB.png" style="height: 100%; max-height: 65px; width: auto;" /></a> &nbsp; &nbsp;
	  </p-->




	<!--p style="text-align: center;">
	<a href="javascript: toggleVisibility ('showall')">Show</a>/<a href="javascript: toggleVisibility ('hideall')">Hide all abstracts</a>
	</p-->


  <!--table class="table table-bordered table-hover table-condensed programTable"> 
    <tbody>
      <tr>
        <th width="17%">Time</th>
        <th width="83%">Event</th>
      </tr>
      <tr>
        <td>8:00 - 8:45</td>
        <td><strong>Registration</strong></td>
      </tr>
      <tr>
        <td>8:45 - 9:00</td>
        <td><strong>Welcome and Acknowledgments</strong></td>
      </tr>

	  <tr class="success">
        <td colspan="2" align="center"><strong>Session 1: Cloud Data Management</strong> Chair: John Liagouris (Boston University)</td>
      </tr>
	  
      <tr>
        <td>9:00 - 9:30</td>
        <td>
		   <strong>Keynote 1</strong>: Stavros Papadopoulos (TileDB) <i>The Adaptive Database</i> <a href="download/slides/TileDB_Adaptive_Intelligence_StavrosPapadopoulos.pdf" target="_blank">[Slides]</a>
		   <br>
          <a href="javascript: toggleVisibility ('#keynote1A')">Click to toggle abstract and bio.</a>
          <div id="keynote1A" class="abstract" style="display: none;">
            <h3>Abstract</h3>
            <p>
			In this talk I will present our work with TileDB, which I created when I was at MIT and Intel Labs and then spun out into its own company (which has raised over $55M to day and counts 75 talented individuals). TileDB is an adaptive database, which morphs to capture all the complex data and complex compute required to solve challenging data & AI problems in numerous applications, all in a single product and with maximum performance and simplicity. It is architected around multi-dimensional arrays, a powerful data structure that shape-shifts to efficiently model all the diverse data of your applications. It also offers vertical functionality for a broad set of challenging application domains. Finally, TileDB is enterprise-grade and has security as a top priority, guaranteeing high-standard governance and compliance. The talk will also cover the challenges we faced and the amazing customer problems we have solved. 
            </p>
            <h3>Bio</h3>
            <p>
			Prior to founding TileDB, Inc. in February 2017, Stavros was a Senior Research Scientist at the Intel Parallel Computing Lab, and a member of the Intel Science and Technology Center for Big Data at MIT CSAIL for three years. He also spent about two years as a Visiting Assistant Professor at the Department of Computer Science and Engineering of the Hong Kong University of Science and Technology (HKUST). Stavros received his PhD degree in Computer Science at HKUST under the supervision of Prof. Dimitris Papadias, and held a postdoc fellow position at the Chinese University of Hong Kong with Prof. Yufei Tao. 
            </p>
          </div>
        </td>
      </tr>

      <tr>
        <td>9:30 - 9:45</td>

        <td>
		  <i>Virtualizing Cloud Data Infrastructures with BRAD</i> <a href="download/slides/Virtualizing_Cloud_Data_Infrastructures_with_BRAD_GeoffreyXYu.pdf" target="_blank">[Slides]</a> <a href="download/posters/Virtualizing_Cloud_Data_Infrastructures_with_BRAD.pdf" target="_blank">[Poster]</a>
		  <br>
		  Geoffrey X. Yu (MIT)*; Ziniu Wu (MIT); Ferdinand Kossmann (MIT); Tianyu Li (MIT CSAIL); Markos Markakis (MIT); Amadou L Ngom (MIT); Tim Kraska (MIT); Samuel Madden (MIT)
		  <br>
          <a href="javascript: toggleVisibility ('#pa1A')">Click to toggle abstract.</a>
          <div id="pa1A" class="abstract" style="display: none;">
            <p>
			In the cloud, organizations typically manage their data using a mishmash of specialized database engines (e.g., Aurora, BigQuery, etc.), each chosen for the functionality and performance/cost benefits they bring to different parts of their workload. Realizing and managing these multi-engine infrastructures is hard. There can be multiple possible designs, each with subtly different performance and costs. Moreover, changing the design later on (e.g., due to business growth) is challenging as application code usually ends up tightly coupled to the chosen physical infrastructure. We solve these problems using virtual database engines (VDBEs), a novel abstraction that we implement in BRAD: a new system that automates data infrastructure design and management. VDBEs virtualize a data infrastructure to separate the concerns of logically specifying the performance/functionality that applications need from physically realizing them onto concrete engines. Users declaratively define a set of VDBEs, which each describe the “properties” that an application expects from an engine (e.g., query interface, performance, data freshness). BRAD then automatically realizes these VDBEs onto physical cloud engines by solving a cost-based optimization problem, leveraging learned performance models. Applications integrate with VDBEs and have no knowledge of the underlying physical engines, enabling BRAD to later modify the physical infrastructure without needing any application code changes. In our talk, we will present the VDBE abstraction and highlight its flexibility by giving examples of how common data infrastructure needs are expressed using VDBEs. We will also discuss how BRAD automatically realizes VDBEs onto a physical infrastructure.

			This talk is based on ongoing work and two papers (one published, one under submission).
            </p>
          </div>
        </td>
      </tr>
		<tr>
		<td>9:45 - 10:00</td>
        <td>
		  <i>Towards Adaptive Transaction Processing in Untrusted Environments</i> <a href="download/slides/Towards_Adaptive_Transaction_Processing_in_Untrusted_Environments_MohammadJavadAmiri.pdf" target="_blank">[Slides]</a> 
		  <br>
		  Mohammad Javad Amiri (Stony Brook University)*
		  <br>
          <a href="javascript: toggleVisibility ('#pa1C')">Click to toggle abstract.</a>
          <div id="pa1C" class="abstract" style="display: none;">
            <p>
			This talk articulates our vision for a learning-based distributed transaction processing framework that can be deployed in untrustworthy environments. We argue that as novel smart contracts, modern hardware, and new cloud platforms arise, future-proof distributed transaction processing systems need to be designed with full-stack adaptivity in mind. At the application level, a future-proof system must adaptively learn the best-performing transaction processing paradigm and quickly adapt to new hardware and unanticipated workload changes on the fly. Likewise, the consensus layer must dynamically adjust itself to the workloads, faulty conditions, and network configuration while maintaining compatibility with the transaction processing paradigm. At the infrastructure level, cloud providers must enable cross-layer adaptation, which identifies performance bottlenecks and possible attacks, and determines at runtime the degree of resource disaggregation that best meets application requirements. Within this vision of the future, this talk outlines several research challenges together with some preliminary approaches.
            </p>
          </div>
        </td>
      </tr>
		<tr>
		<td>10:00 - 10:15</td>
        <td>
		  <i>Self-Organizing Data Containers</i> <a href="download/slides/Self-Organizing_Data_Containers_SivaprasadSudhir.pdf" target="_blank">[Slides]</a> <a href="download/posters/Self-Organizing_Data_Containers.pdf" target="_blank">[Poster]</a>
		  <br>
		  Sivaprasad Sudhir (MIT)*; Jialin Ding (Amazon Web Services); Tim Kraska (MIT); Samuel Madden (MIT)
		  <br>
          <a href="javascript: toggleVisibility ('#pa1D')">Click to toggle abstract.</a>
          <div id="pa1D" class="abstract" style="display: none;">
            <p>
			We present a new self-organizing, self-optimizing, meta-data-rich storage layer for the cloud called self-organizing data containers (SDCs). SDCs enable order-of-magnitude performance improvements in data-intensive applications through instance optimization, i.e., the adaptation of data representation to exploit both the distribution of the data and the workload running on it. SDCs support modern data storage optimizations at the heart of high-performance data analytics systems, such as indexing, flexible multi-dimensional partitioning and compression, and an ability to adapt to the workloads running on them. Once we have SDCs, many existing
			systems, including conventional relational databases, parallel data processing frameworks, and ML systems, can
			easily adapt SDCs as their storage layer and attain huge performance gains.
            </p>
          </div>
        </td>
      </tr>
	  
      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  
      <tr>
        <td>10:15 - 10:45</td>
        <td><strong>Coffee Break</strong></td>
      </tr>

      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  

	  <tr class="success">
        <td colspan="2" align="center"><strong>Session 2: Graphs</strong> Chair: Vasia Kalavri (Boston University)</td>
      </tr>
	  
       <tr>
        <td>10:45 - 11:15</td>
        <td>
		   <strong>Keynote 2</strong>: Paul Dix (InfluxDB) <i>The Architecture of InfluxDB 3: A Distributed, Real-time Database on Object Storage</i> <a href="download/slides/The_Architecture_of_InfluxDB_3.0_PaulDix.pdf" target="_blank">[Slides]</a>
		   <br>
          <a href="javascript: toggleVisibility ('#keynote1B')">Click to toggle abstract and bio.</a>
          <div id="keynote1B" class="abstract" style="display: none;">
            <h3>Abstract</h3>
            <p>
			This talk will take an in depth look at the architecture of InfluxDB 3, a distributed real-time, time series database that uses object store as its historical durability layer. I'll cover the specific components and design considerations on separating ingestion from compaction from query workloads. We'll also look at some of the challenges encountered when building a database on top of object storage that requires sub-second query response times with immediate read-your-writes capability. 
            </p>
            <h3>Bio</h3>
            <p>
			Paul is the creator of InfluxDB. He has helped build software for startups, large companies and organizations like Microsoft, Google, McAfee, Thomson Reuters, and Air Force Space Command. He is the series editor for Addison Wesley’s Data & Analytics book and video series. In 2010 Paul wrote the book Service Oriented Design with Ruby and Rails for Addison Wesley. In 2009 he started the NYC Machine Learning Meetup which has hosted over 100 speakers from academia, research, and industry over the years. Paul holds a degree in computer science from Columbia University. 
            </p>
          </div>
        </td>
      </tr>

      <tr>
		<td>11:15 - 11:30</td>
        <td>
		  <i>CAVE: Concurrency-Aware Graph Processing System for SSD</i> <a href="download/slides/CAVE_Concurrency-Aware_Graph_Processing_System_for_SSD_MdTarikulIslamPapon.pdf" target="_blank">[Slides]</a> <a href="download/posters/CAVE_Concurrency-Aware_Graph_Processing_System_for_SSD.pdf" target="_blank">[Poster]</a>
		  <br>
		  Tarikul Islam Papon (Boston University)*; Manos Athanassoulis (Boston University)
		  <br>
          <a href="javascript: toggleVisibility ('#pa2C')">Click to toggle abstract.</a>
          <div id="pa2C" class="abstract" style="display: none;">
            <p>
			Large-scale graph analytics has become increasingly common in areas like social networks, physical sciences, transportation networks, and recommendation systems. Since many such practical graphs do not fit in main memory, graph analytics performance depends on efficiently utilizing underlying storage devices. These out-of-core graph processing systems employ sharding and sub-graph partitioning to optimize for storage while relying on efficient sequential access of traditional hard disks. However, today’s storage is increasingly based on solid-state drives (SSDs) that exhibit high internal parallelism and efficient random accesses. Yet, state-of-the-art graph processing systems do not explicitly exploit those properties, resulting in subpar performance.
			We develop CAVE, a graph processing engine that exploits the underlying SSD-based storage by harnessing the available device parallelism via carefully selecting which I/Os to graph data can be issued concurrently. Thus, CAVE traverses multiple paths and processes multiple nodes and edges concurrently. We identify two key ways to parallelize graph traversal algorithms based on the graph structure and algorithm: intra-subgraph and inter-subgraph parallelization. To showcase the benefit of our approach, we build within CAVE parallelized versions of five popular graph algorithms (Breadth-First Search, Depth-First Search, Weakly Connected Components, PageRank, Random Walk) that exploit the full bandwidth of the underlying device. CAVE uses a blocked file format based on adjacency lists and employs a concurrent cache pool. By experimenting with different types of graphs on three SSD devices, we demonstrate that CAVE utilizes the available parallelism, and scales to diverse real-world graph datasets. CAVE achieves up to one order of magnitude speedup compared to the popular out-of-core systems Mosaic and GridGraph, and up to three orders of magnitude speedup in runtime compared to GraphChi.
            </p>
          </div>
        </td>
      </tr>

      <tr>
        <td>11:30 - 11:45</td>
        <td>
		  <i>Split-Parallel Graph Neural Network Training</i>
		  <br>
		  Sandeep Polisetty (UMASS)*; Juelin Liu (University of Massachusetts, Amherst); Hui Guan (University of Massachusetts, Amherst); Marco Serafini (University of Massachusetts, Amherst) <a href="download/slides/Split-Parallel_Graph_Neural_Network_Training_SandeepPolisetty.pdf" target="_blank">[Slides]</a> <a href="download/posters/Split-Parallel_Graph_Neural_Network_Training.pdf" target="_blank">[Poster]</a>
		  <br>
          <a href="javascript: toggleVisibility ('#pa2D')">Click to toggle abstract.</a>
          <div id="pa2D" class="abstract" style="display: none;">
            <p>
			Graph neural networks (GNNs), an emerging class
			of machine learning models for graphs, have gained popularity
			for their superior performance in various graph analytical tasks.
			Mini-batch training is commonly used to train GNNs on large
			graphs, and data parallelism is the standard approach to scale
			mini-batch training across multiple GPUs. One of the major
			performance costs in GNN training is the loading of input features,
			which prevents GPUs from being fully utilized. In this talk, we
			argue that this problem is exacerbated by redundancies that are
			inherent to the data parallel approach. To address this issue, we
			introduce a hybrid parallel mini-batch training paradigm called
			split parallelism. Split parallelism avoids redundant data loads
			and splits the training of each mini-batch across multiple GPUs
			online, at each iteration, using a lightweight splitting algorithm.
			We implement split parallelism in a novel system named Spara
			and show that it outperforms state-of-the-art mini-batch training
			systems like DGL, Quiver, and P3 
			</p>
		  </div>
        </td>
      </tr>


      <tr>
	    <td>11:45 - 12:00</td>
        <td>
		  <i>GNN Training Systems: Comparison of Full-Graph and Mini-Batch</i> <a href="download/slides/GNN_Training_Systems_Comparison_of_Full-Graph_and_Mini-Batch_SaurabhBalkishanBajaj.pdf" target="_blank">[Slides]</a> <a href="download/posters/GNN_Training_Systems_Comparison_of_Full-Graph_and_Mini-Batch.pdf" target="_blank">[Poster]</a>
		  <br>
		  Saurabh Bajaj (University of Massachusetts, Amherst)*; Hui Guan (University of Massachusetts, Amherst); Marco Serafini (University of Massachusetts, Amherst)
		  <br>
          <a href="javascript: toggleVisibility ('#pa2E')">Click to toggle abstract.</a>
          <div id="pa2E" class="abstract" style="display: none;">
            <p>
			Graph Neural Networks (GNNs) have gained significant
			attention in recent years due to their ability to learn representations of graph-structured data. Two common methods for training GNNs are mini-batch training and full-graph training. 
			Since these two methods require different training pipelines and systems optimizations, two separate categories of GNN training systems emerged, each tailored for one method only. 
			Previous research introducing systems belonging to a particular category predominantly compares them with other systems within the same category, offering limited or no comparison with systems from the other category.
			Some prior work also argues that one training method is superior to the other to justify their focus on that method, but the existing evidence is incomplete and contradictory. In this paper, we empirically compare the two training methods and GNN training systems that support them in terms of accuracy and performance.
			Our experiments demonstrate that both training techniques converge to similar accuracy values across multiple datasets and models, with a slight advantage for full-graph training.
			Mini-batch training is less sensitive to model architecture changes, making hyperparameter search easier.
			The mini-batch training systems we consider consistently converge faster than the full-graph training ones across multiple datasets, GNN models, and system configurations, between 2.4x - 15.2x faster.

			The talk is about my current work.
            </p>
          </div>
        </td>
      </tr>
	  
      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  
      <tr>
        <td>12:00 - 1:00</td>
        <td><strong>Lunch Break</strong></td>
      </tr>

      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  
	  <tr class="success">
        <td colspan="2" align="center"><strong>Session 3: Potpourri (From Causality to Dynamization)</strong> Chair: Tarikul Islam Papon (Boston University)</td>
      </tr>
	  
      <tr>
		<td>1:00 - 1:15</td>
        <td>
		  <i>Sawmill: From Logs to Causal Diagnosis of Large Systems</i> <a href="download/slides/Sawmill_From_Logs_to_Causal_Diagnosis_of_Large_Systems_MarkosMarkakis.pdf" target="_blank">[Slides]</a> <a href="download/posters/Sawmill_From_Logs_to_Causal_Diagnosis_of_Large_Systems.pdf" target="_blank">[Poster]</a>
		  <br>
		  Markos Markakis (MIT)*; An Bo Chen (MIT CSAIL); Brit Youngmann (Technion - Israel institute of technology); Trinity Gao (MIT); Ziyu Zhang (MIT); Rana Shahout (Harvard); Peter Chen (MIT); Chunwei Liu (MIT); Ibrahim Sabek (University of Southern California); Michael Cafarella (MIT CSAIL)
		  <br>
          <a href="javascript: toggleVisibility ('#pa2F')">Click to toggle abstract.</a>
          <div id="pa2F" class="abstract" style="display: none;">
            <p>
			Causal analysis is an essential lens for understanding complex system dynamics in domains as varied as medicine, economics and law. Computer systems can exhibit similar complexity, but much of the information that could help subject them to causal analysis is only available in long, messy, semi-structured log files.

			In this work, we want to apply causal reasoning to facilitate large systems debugging. We transform logs into a representation amenable to causal analysis, using methods from data transformation, cleaning, and extraction. We also present algorithms for efficiently using this representation for causal discovery - the task of constructing a causal model of the system from available data. Our framework gives log-derived variables human-understandable names and distills the information in a log file around a user’s chosen causal units (e.g. users or machines), generating appropriate aggregated variables per causal unit. It then exposes these variables through an interactive interface that users can leverage to recover the portion of the causal model relevant to their question. This makes causal inference possible using off-the-shelf tools.

			We evaluate our framework using Sawmill, an open-source prototype implementation, on both real-world and synthetic log datasets and find it to be: 1. Accurate, achieving on average across datasets a mean reciprocal rank of 0.8018 for relevant causes (41.89% better than the next best baseline) and a mean ATE error of 20.27% (compared to 31.26% for the next best baseline); 2. Computationally efficient, requiring a mean of 346.92s (4.30s per MiB of log data) to go from a log file to a quantitative effect, while scaling linearly with log complexity; 3. Humanly efficient, requiring between 6-10 user interactions end-to-end; and 4. Complete, with each component of our framework shown to both add value and to be efficient. A demonstration of Sawmill has been accepted to SIGMOD ’24. A companion video is available online.
            </p>
          </div>
        </td>
      </tr>
	  <tr>
		<td>1:15 - 1:30</td>
        <td>
		  <i>StarfishDB: Probabilistic Programming Datalog in Action</i> <a href="download/slides/StarfishDB_Probabilistic_Programming_Datalog in_Action_NiccoloMeneghetti.pdf" target="_blank">[Slides]</a--> <!--a href="download/posters/StarfishDB_Probabilistic_Programming_Datalog in_Action.pdf" target="_blank">[Poster]</a-->
		  <!--br>
		  Niccolo Meneghetti (University of Michigan - Dearborn)*
		  <br>
          <a href="javascript: toggleVisibility ('#pa3C')">Click to toggle abstract.</a>
          <div id="pa3C" class="abstract" style="display: none;">
            <p>
			StarfishDB is a database-centric probabilistic programming framework, where probabilistic programs are expressed as sets of relational constraints, imposed against a probabilistic database. In this talk we will discuss the design and implementation of StarfishDB, and some of its practical applications. The talk is based on a current research paper, accepted at SIGMOD 2024.
            </p>
          </div>
        </td>
      </tr>


      <tr>
		<td>1:30 - 1:45</td>
        <td>
		  <i>Solving Reverse Data Management Problems as Fast as Theoretically Possible: A Unified Framework</i> <a href="download/slides/Solving_Reverse_Data_Management_Problems_as_Fast_as_Theoretically_Possible_A_Unified_Framework_NehaMakhija.pdf" target="_blank">[Slides]</a> <a href="download/posters/Solving_Reverse_Data_Management_Problems_as_Fast_as_Theoretically_Possible_A_Unified_Framework.pdf" target="_blank">[Poster]</a>
		  <br>
		  Neha Makhija (Northeastern University)*; Wolfgang Gatterbauer (Northeastern University)
		  <br>
          <a href="javascript: toggleVisibility ('#pa2G')">Click to toggle abstract.</a>
          <div id="pa2G" class="abstract" style="display: none;">
            <p>
			Reverse Data Management (RDM) problems are those that ask: “What minimum set of interventions on a database produces a certain (desired) change in the output of a query?.” Many important problems such as deletion propagation, fairness testing, debugging, provenance factorization, causal responsibility, and resilience can be cast as RDM problems. We use a novel approach to solve RDM problems: instead of creating dedicated algorithms for easy (PTIME) and hard cases (NP-complete), we devise unified algorithms that are guaranteed to terminate in PTIME for easy cases. This approach allows us to separate the task of devising efficient algorithms and proving tractability, allowing us to solve problems “as fast as theoretically possible” and to automatically recover easy instances for generally hard problems. This leads us to both practical benefits in terms of automatic time guarantees and ease of implementation as well as new theoretical benefits such as new complexity results, and an automatic hardness gadget finder. This talk will present our unified framework and its application to the problems of resilience, causal responsibility, and minimal factorization of provenance (from work to be presented at SIGMOD 24 and PODS 24).
            </p>
          </div>
        </td>
      </tr>

      <tr>
		<td>1:45 - 2:00</td>
        <td>
		  <i>Semantic Version Management in Data Lakes</i> <a href="download/slides/DataVersioning_RoeeShraga.pdf" target="_blank">[Slides]</a>
		  <br>
		  Roee Shraga (Worcester Polytechnic Institute)*
		  <br>
          <a href="javascript: toggleVisibility ('#pa2H')">Click to toggle abstract.</a>
          <div id="pa2H" class="abstract" style="display: none;">
            <p>
			*The talk is based on a published paper and open problems that follow*

			In this talk, I will present our new project on Semantic Version Management in which we seek to lay the foundations for semantic understanding of data changes that result in multiple versions and introduce scalable tools to uncover and explain data changes. This project is supported by NSF award number IIS-2325632. Specifically, I will introduce Explain-Da-V (a VLDB23 Paper), a framework that explains changes between two given dataset versions using data transformations. I will present open problems and challenges that include efficiently finding different versions of a dataset from within a massive table repository and understanding the version history among a collection of versions. This is a joint work with Renee Miller from Northeastern University.
            </p>
          </div>
        </td>
      </tr>

      <tr>
		<td>2:00 - 2:15</td>
        <td>
		  <i>Why aren’t you dynamizing your data structures?</i> <a href="download/slides/Why_aren't_you_dynamizing_your_data_structures_DouglasBahrRumbaugh.pdf" target="_blank">[Slides]</a--> <!--a href="download/posters/Why_aren't_you_dynamizing_your_data_structures.pdf" target="_blank">[Poster]</a-->
		  <!--br>
		  Douglas B Rumbaugh (Penn State University)*; Dong Xie (Penn State University)
		  <br>
          <a href="javascript: toggleVisibility ('#pa2I')">Click to toggle abstract.</a>
          <div id="pa2I" class="abstract" style="display: none;">
            <p>
			It is not uncommon for the most effective data structure for answering
			a particular type of query to be static, but unfortunately update
			support is often desirable. This talk will discuss classical theoretical
			techniques for automatically adding update support to static structures
			(dynamization), the problems that make these techniques unpopular in
			practice, and approaches to fixing those problems. Dynamization has
			the potential to greatly improve the utility of existing static data
			structures, and we'll show a few specific examples of just how powerful
			it can be when properly applied, as well as discuss approaches to improve
			the performance and generality of the techniques.
            </p>
          </div>
        </td>
      </tr>

      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  
      <tr>
        <td>2:15 - 2:45</td>
        <td><strong>Coffee Break</strong></td>
      </tr>

      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>



      <tr class="success">
        <td colspan="2" align="center"><strong>Session 4: ML & Data Systems</strong> Chair: Andy Huynh (Boston University)</td>
      </tr>
	  
       <tr>
        <td>2:45 - 3:15</td>
        <td>
		   <strong>Keynote 3</strong>: Immanuel Trummer (Cornell University) <i>Novel Applications for Large Language Models in Data Management</i> <a href="download/slides/Novel_Applications_for_Large_Language_Models_in_Data_Management_ImmanuelTrummer.pdf" target="_blank">[Slides]</a>
		   <br>
          <a href="javascript: toggleVisibility ('#keynote4A')">Click to toggle abstract and bio.</a>
          <div id="keynote4A" class="abstract" style="display: none;">
            <h3>Abstract</h3>
            <p>
			The past years have been marked by several breakthrough results in the domain of generative AI, culminating in the rise of tools like ChatGPT, able to solve a variety of language-related tasks without specialized training. In this talk, I outline novel opportunities in the context of data management, enabled by these advances. I discuss several recent research projects at Cornell, aimed at exploiting advanced language processing for tasks such as parsing text about databases and data sets to support automated index selection, parameter tuning, or data profiling, mining data for patterns, described in natural language, or synthesizing scenario-specific code for data processing via specialized coding assistants. Finally, I discuss recent projects, aimed at scaling up the analysis of multimodal data via large language models to large repositories. 
            </p>
            <h3>Bio</h3>
            <p>
			Immanuel Trummer is an assistant professor at Cornell University and a member of the Cornell Database Group. His papers were selected for “Best of VLDB”, “Best of SIGMOD”, for the ACM SIGMOD Research Highlight Award, and for publication in CACM as CACM Research Highlight. His online lecture introducing students to database topics collected over a million views. He received the NSF CAREER Award and multiple Google Faculty Research Awards. 
            </p>
          </div>
        </td>
      </tr>


      <tr>
        <td>3:15 - 3:30</td>
        <td>
		  <i>Why Machine Learning for Automatically Optimizing Databases Doesn’t Work</i>
		  <br>
		  Andrew Pavlo (Carnegie Mellon University)*
		  <br>
          <a href="javascript: toggleVisibility ('#pa3A')">Click to toggle abstract.</a>
          <div id="pa3A" class="abstract" style="display: none;">
            <p>
			Database management systems (DBMSs) are complex software that requires sophisticated tuning to work efficiently for a given workload and operating environment. Such tuning requires considerable effort from experienced administrators, which is not scalable for large DBMS fleets. This problem has led to research on using machine learning (ML) to devise strategies to optimize DBMS configurations for any application, including automatic physical database design, knob configuration, and query tuning. Despite the many academic papers that tout the benefits of using ML to optimize databases, there have been only a few major success stories in industry in the last decade.

			In this talk, I discuss the challenges of using ML-enhanced tuning methods to optimize databases. I will address specific assumptions that researchers make about production database environments that are incorrect and identify why ML is not always the best solution to solving real-world database problems.
            </p>
          </div>
        </td>
      </tr>




      <tr>
        <td>3:30 - 3:45</td>
        <td>
		  <i>Towards an Industry-Scale Feature Engineering Benchmark</i> <a href="download/slides/Towards_an_Industry-Scale_Feature_Engineering_Benchmark_AbdulWasay.pdf" target="_blank">[Slides]</a> <a href="download/posters/Towards_an_Industry-Scale_Feature_Engineering_Benchmark.pdf" target="_blank">[Poster]</a>
		  <br>
		  Abdul Wasay*; Sounak Gupta; Nesime Tatbul (MIT); David E Cohen; Orri Erling (Meta Platforms)
		  <br>
          <a href="javascript: toggleVisibility ('#pa3B')">Click to toggle abstract.</a>
          <div id="pa3B" class="abstract" style="display: none;">
            <p>
			Feature engineering is integral to all machine learning pipelines. It is the process through which these pipelines transform raw data into features that are then stored and used for several state-of-the-art machine learning tasks, for instance, training and inferring from deep learning recommendation models (DLRMs) and fine-tuning NLP models through retrieval-augmented generation (RAG). Feature engineering consumes a significant portion of resources at large-scale companies; for instance, it accounts for over 30 percent of the power consumption of production DLRM pipelines at Meta. Feature engineering pipelines at large-scale companies such as Meta, Pinterest, and Amazon exhibit unique properties including a highly disaggregated hardware/software stack, specialized nested data types, massive data about user browsing history, and high refresh rate. Existing benchmarks such as TPC-DS or FEBench do not capture all of these characteristics within the same benchmark. In this talk, we describe a joint effort to build a feature engineering benchmark that closely captures the scale, data types, and query workloads of data pipelines at various large-scale companies.
            </p>
          </div>
        </td>
      </tr>




      <tr>
        <td>3:45 - 4:00</td>
        <td>
		  <i>Specialized vs. Generalized Vector Databases: How Different Are They Really?</i> <a href="download/slides/Specialized_vs_Generalized_Vector_Databases_JianguoWang.pdf" target="_blank">[Slides]</a>
		  <br>
		  Jianguo Wang (Purdue University)*
		  <br>
          <a href="javascript: toggleVisibility ('#pa3D')">Click to toggle abstract.</a>
          <div id="pa3D" class="abstract" style="display: none;">
            <p>
			Recently, there has been a tremendous surge of interest in building vector databases. Existing vector databases can be broadly classified into two categories: specialized and generalized vector databases. Our experiments demonstrate a significant performance slowdown in generalized vector databases compared to specialized ones. This outcome is expected, as specialized vector databases can leverage optimizations not available to generalized vector databases. But what exactly are the root causes? Is it possible to bridge the performance gap? If so, how?

			In this talk, I will present our research answering these questions. This talk is based on a collection of papers published or submitted: ICDE'24, VLDB'24 (submitted), TODS'24 (submitted), and SIGMOD'21.
            </p>
          </div>
        </td>
      </tr>
	  
      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  
      <tr>
        <td>4:00 - 4:30</td>
        <td><strong>Coffee Break & Poster Preparation</strong></td>
      </tr>
	  
      <tr>
        <td colspan="2">&nbsp;</td>
      </tr>

      <tr class="success">
        <td>4:30 - 6:30</td>
        <td>
		<i><strong><a href="#posters">Poster Session (CCDS 2nd floor)</a></strong></i>
        </td>
      </tr>
	  
	  <tr>
        <td colspan="2">&nbsp;</td>
      </tr>
	  
	  <tr class="success">
        <td>6:30 - 8:30</td>
        <td>
		<i><strong>Student Pizza Networking Session (CCDS 17th floor)</strong></i>
        </td>
      </tr-->

<!--
      <tr>
        <td>6.00-7.30</td>
        <td>
  <i>Event Name (CCDS 1750) </i>

        </td>
      </tr>
-->
       <!--tr>
        <td colspan="2">&nbsp;</td>
      </tr>
  <tr class="success">
        <td colspan="2" align="center"><strong>End Of Official Program</strong></td>
      </tr>


    </tbody>
  </table-->

</div>
